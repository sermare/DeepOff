{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "733f8cec",
   "metadata": {},
   "source": [
    "# BERT - CRISPRoff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ea3563",
   "metadata": {},
   "source": [
    "Tutorial adapted from https://colab.research.google.com/github/antonio-f/BERT_from_scratch/blob/main/BERT_from_scratch.ipynb#scrollTo=qwf2L-wZFJo0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c63fa6",
   "metadata": {},
   "source": [
    "### 0. Useful links\n",
    "\n",
    "https://datascience.stackexchange.com/questions/54412/how-to-add-a-cnn-layer-on-top-of-bert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165d95b9",
   "metadata": {},
   "source": [
    "### 1. Packages and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af4bedb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "n_sequences = 10\n",
    "kmer_length = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "461719fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from subprocess import PIPE, run\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import pprint\n",
    "import requests\n",
    "\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "Data = pd.read_csv('/Users/sergiomares/Desktop/Nunez/Jin file/TSS_CpG_crispriphenotype_table.txt', delimiter = '\\t',header = 0)\n",
    "\n",
    "b = []\n",
    "\n",
    "for i in range (n_sequences):\n",
    "    datas = requests.get('http://togows.org/api/ucsc/hg19/'+ str(Data['chromosome'][i])+':'+str(int(Data[\"Primary TSS, 5'\"][i]-1000))+'-'+str(Data[\"Primary TSS, 5'\"][i]+1000)).text.replace('\\n','')    \n",
    "    b.append(datas)\n",
    "\n",
    "Promoter_sequences = pd.DataFrame(b)\n",
    "\n",
    "def getKmers(sequence, size=6):\n",
    "    return [sequence[x:x+size].lower() for x in range(len(sequence) - size + 1)]\n",
    "\n",
    "Promoter_sequences = Promoter_sequences.apply(lambda x: getKmers(x[0], kmer_length), axis = 1)\n",
    "Promoter_sequences = pd.DataFrame(Promoter_sequences)\n",
    "\n",
    "Promoters_text = list(Promoter_sequences[0])\n",
    "\n",
    "for item in range(len(Promoter_sequences)):\n",
    "    Promoters_text[item] = ' '.join(Promoter_sequences[0][item])\n",
    "\n",
    "y_data = Promoter_sequences.iloc[:, 0].values     \n",
    "\n",
    "Promoter_sequences[1] = Data.index[:(len(Promoter_sequences))]\n",
    "Promoter_sequences.columns = [['sequence', 'index']]\n",
    "Promoter_sequences['text'] = pd.DataFrame(Promoters_text)\n",
    "\n",
    "#dataset = dict(pd.MultiIndex.from_frame(Promoter_sequences[['index','text']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e6988a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: [\"('index',)\", \"('text',)\"],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "datasets = Dataset.from_pandas(Promoter_sequences[['index','text']])\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47b13e4",
   "metadata": {},
   "source": [
    "### 2. Building a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "702e6d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_text(dataset, output_filename=\"data.txt\"):\n",
    "  \"\"\"Utility function to save dataset text to disk,\n",
    "  useful for using the texts to train the tokenizer \n",
    "  (as the tokenizer accepts files)\"\"\"\n",
    "  with open(output_filename, \"w\") as f:\n",
    "    for t in dataset:\n",
    "      print(t, file=f)\n",
    "\n",
    "dataset_to_text(datasets, \"train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9e468dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "tokenizer.train(files='train.txt', \n",
    "                vocab_size=30_522,\n",
    "                min_frequency=6,\n",
    "                special_tokens=['<s>', '<pad>', '</s>', '<unk>', '<mask>']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c91363ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['liberto/vocab.json', 'liberto/merges.txt']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "os.mkdir('./liberto')\n",
    "tokenizer.save_model('liberto')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce4537e",
   "metadata": {},
   "source": [
    "### 3. Initializing the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "22252915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('liberto',max_length = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "05729852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 277, 277, 2]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer('gcgc')\n",
    "tokens.input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabf4bc8",
   "metadata": {},
   "source": [
    "### 4. Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4beeb72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tokenizer( Promoters_text , max_length=10, padding='max_length', truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "72d38609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "labels = torch.tensor([x for x in batch['input_ids']])\n",
    "mask = torch.tensor([x for x in batch['attention_mask']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c0c781f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,   88,  284,  261,  580,  584,  543,  861,  341,    2],\n",
       "        [   0,  271,  305,  613,  541,  790,  414,  521,  432,    2],\n",
       "        [   0,  262,  353,  431,  736,  761, 1220, 1110, 1217,    2],\n",
       "        [   0,   88,   69,  312,  802, 1076,  866,  684,  512,    2],\n",
       "        [   0,  284,  283,  422,  382,  392,  783,  492,  617,    2],\n",
       "        [   0,  286,  271,  925, 1122,  511,  529,  526,  688,    2],\n",
       "        [   0,   88,   71,  351,  427,  325,  288,  388, 1250,    2],\n",
       "        [   0,  262,  285,  444,  494,  514,  493,  557,  426,    2],\n",
       "        [   0,  287,  261, 1012,  681,  622,  517,  621, 1263,    2],\n",
       "        [   0,  319,  262, 1286, 1030,  450,  892,  608,  537,    2]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c9bc5b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make copy of labels tensor, this will be input_ids\n",
    "input_ids = labels.detach().clone()\n",
    "# create random array of floats with equal dims to input_ids\n",
    "rand = torch.rand(input_ids.shape)\n",
    "# mask random 15% where token is not 0 [PAD], 1 [CLS], or 2 [SEP]\n",
    "# mask_arr = (rand < .15) * (input_ids != 0) * (input_ids != 1) * (input_ids != 2)\n",
    "mask_arr = (rand < .15) * (input_ids > 2) \n",
    "# loop through each row in input_ids tensor (cannot do in parallel)\n",
    "for i in range(input_ids.shape[0]):\n",
    "    # get indices of mask positions from mask array\n",
    "    selection = torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "    # mask input_ids\n",
    "    input_ids[i, selection] = 3  # our custom [MASK] token == 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9391248b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4b0dd14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = {'input_ids': input_ids, 'attention_mask': mask, 'labels': labels} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "da590a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        # store encodings internally\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        # return the number of samples\n",
    "        return self.encodings['input_ids'].shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # return dictionary of input_ids, attention_mask, and labels for index i\n",
    "        return {key: tensor[i] for key, tensor in self.encodings.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "306f36a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(encodings) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "183f6567",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562f998e",
   "metadata": {},
   "source": [
    "### 5. Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5f169e",
   "metadata": {},
   "source": [
    "#### A. Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6549e4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=30_522,  # we align this to the tokenizer vocab_size\n",
    "    max_position_embeddings=514,\n",
    "    hidden_size=768,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ed44a656",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "model = RobertaForMaskedLM(config) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4011086c",
   "metadata": {},
   "source": [
    "#### B. Training Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ecfcfc7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForMaskedLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# and move our model over to the selected device\n",
    "model.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3cb2561e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergiomares/miniconda3/envs/tf/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# activate training mode\n",
    "model.train()\n",
    "# initialize optimizer\n",
    "optim = AdamW(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ec0058",
   "metadata": {},
   "source": [
    "#### C. Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f30e72b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:01<00:00,  1.29s/it, loss=10.5]\n",
      "Epoch 1: 100%|██████████| 1/1 [00:01<00:00,  1.61s/it, loss=8.85]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # setup loop with TQDM and dataloader\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    for batch in loop:\n",
    "        # initialize calculated gradients (from prev step)\n",
    "        optim.zero_grad()\n",
    "        # pull all tensor batches required for training\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        # process\n",
    "        outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                        labels=labels)\n",
    "        # extract loss\n",
    "        loss = outputs.loss\n",
    "        # calculate loss for every parameter that needs grad update\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "        # print relevant info to progress bar\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ef9e1f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./liberto')  # and don't forget to save liBERTo!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d60e255",
   "metadata": {},
   "source": [
    "### 6. The real test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ec6c1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "07630e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates file too large for github to sync\n",
    "#fill = pipeline('fill-mask', model='liberto', tokenizer='liberto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1de63cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.0016671427292749286,\n",
       "  'token': 0,\n",
       "  'token_str': '<s>',\n",
       "  'sequence': 'gcgc tata '},\n",
       " {'score': 0.00134976115077734,\n",
       "  'token': 2,\n",
       "  'token_str': '</s>',\n",
       "  'sequence': 'gcgc tata '},\n",
       " {'score': 0.00031376106198877096,\n",
       "  'token': 262,\n",
       "  'token_str': 'aa',\n",
       "  'sequence': 'gcgcaa tata '},\n",
       " {'score': 0.00026194300153292716,\n",
       "  'token': 15426,\n",
       "  'token_str': '',\n",
       "  'sequence': 'gcgc tata '},\n",
       " {'score': 0.00023920593957882375,\n",
       "  'token': 11941,\n",
       "  'token_str': '',\n",
       "  'sequence': 'gcgc tata '}]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill(f'gcgc {fill.tokenizer.mask_token} tata ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7ca7fe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do: Fork ENFORMER from Deepmind repo and adapt it to include methylation information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9809d3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysam\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9b45f634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HS1:329:C3EGUACXX:5:2107:3073:68585\t256\t#0\t10065\t1\t50M\t*\t0\t0\tCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCAACACA\tarray('B', [34, 34, 34, 37, 37, 37, 37, 37, 39, 39, 38, 39, 39, 41, 41, 40, 41, 40, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 40, 40, 40, 40, 41, 41, 41, 41, 41, 40, 41, 41, 41, 41, 41, 40, 40, 41, 41, 41, 41, 41])\t[('AS', -12), ('XN', 0), ('XM', 2), ('XO', 0), ('XG', 0), ('NM', 2), ('MD', '47C1T0'), ('YT', 'UU'), ('NH', 4), ('CC', 'chr10'), ('CP', 135524461), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:2312:18115:5456\t272\t#0\t10532\t0\t50M\t*\t0\t0\tACAGTACCACCGAAATCTGTGCAGAGGAGAACGCAGCTCCGCCCTCGCGG\tarray('B', [36, 35, 35, 35, 35, 33, 35, 33, 32, 35, 36, 37, 37, 37, 37, 37, 37, 39, 39, 37, 39, 39, 41, 41, 41, 41, 41, 41, 41, 41, 40, 41, 41, 41, 41, 41, 41, 39, 39, 39, 39, 39, 37, 37, 37, 37, 37, 34, 34, 34])\t[('AS', -11), ('XN', 0), ('XM', 2), ('XO', 0), ('XG', 0), ('NM', 2), ('MD', '1G26C21'), ('YT', 'UU'), ('NH', 7), ('CC', 'chr12'), ('CP', 95042), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:1212:12394:89987\t256\t#0\t10542\t1\t50M\t*\t0\t0\tCGAAATCTGTGCAGAGGAGAACGCAGCTCCGCCCTCGCGGTGCTCTCCGG\tarray('B', [34, 34, 34, 37, 37, 37, 37, 37, 39, 39, 39, 39, 39, 41, 40, 40, 41, 41, 41, 41, 41, 41, 41, 40, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 38, 39, 40, 41, 39, 39, 39, 39, 39, 37])\t[('AS', -6), ('XN', 0), ('XM', 1), ('XO', 0), ('XG', 0), ('NM', 1), ('MD', '18C31'), ('YT', 'UU'), ('NH', 3), ('CC', 'chr12'), ('CP', 95032), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:2214:16026:87822\t16\t#0\t10547\t1\t50M\t*\t0\t0\tTCTGTGCAGAGGAGAACGCAGCTCCGCCCTCGCGGTGCTCTCCGGGTCTG\tarray('B', [35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 33, 35, 35, 35, 36, 35, 35, 37, 39, 41, 40, 41, 41, 41, 41, 41, 40, 41, 41, 41, 41, 41, 39, 39, 38, 38, 39, 37, 37, 37, 37, 37, 34, 34, 31])\t[('AS', -5), ('XN', 0), ('XM', 1), ('XO', 0), ('XG', 0), ('NM', 1), ('MD', '13C36'), ('YT', 'UU'), ('NH', 3), ('CC', 'chr12'), ('CP', 95027), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:2214:10554:48059\t272\t#0\t10560\t3\t50M\t*\t0\t0\tGAACGCAGCTCCGCCCTCGCGGTGCTCTCCGGGTCTGTGCTGAGGAGAAC\tarray('B', [33, 35, 35, 31, 34, 34, 35, 33, 33, 29, 33, 39, 32, 32, 33, 31, 21, 38, 38, 40, 38, 40, 40, 37, 41, 41, 41, 41, 41, 40, 41, 41, 40, 40, 38, 38, 41, 39, 38, 39, 39, 39, 37, 37, 37, 37, 37, 31, 31, 27])\t[('AS', -5), ('XN', 0), ('XM', 1), ('XO', 0), ('XG', 0), ('NM', 1), ('MD', '0C49'), ('YT', 'UU'), ('NH', 2), ('CC', 'chr15'), ('CP', 102520475), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:2114:9257:17269\t256\t#0\t12025\t0\t50M\t*\t0\t0\tAAACTGCTGGCCTGTGCCAGGGTGGAAGCTGAGCACTGGAGTGGAGTTTT\tarray('B', [16, 10, 16, 19, 19, 10, 17, 32, 35, 37, 39, 37, 39, 40, 38, 40, 38, 38, 34, 39, 40, 40, 30, 38, 33, 37, 38, 34, 39, 38, 35, 35, 39, 38, 40, 40, 40, 40, 40, 40, 40, 35, 37, 39, 39, 40, 40, 40, 40, 39])\t[('AS', -8), ('XN', 0), ('XM', 2), ('XO', 0), ('XG', 0), ('NM', 2), ('MD', '0C23C25'), ('YT', 'UU'), ('XS', '+'), ('NH', 7), ('CC', 'chr15'), ('CP', 102519096), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:1314:19520:90553\t256\t#0\t13141\t0\t50M\t*\t0\t0\tCTGAGGCTGAGGAAGGAGAAGGGGATGCACTGTTGGGGAGGCAGCTGTAA\tarray('B', [34, 34, 34, 37, 37, 37, 37, 37, 39, 39, 39, 39, 39, 40, 41, 41, 39, 40, 40, 40, 40, 41, 41, 41, 39, 39, 40, 41, 35, 38, 39, 39, 40, 41, 41, 41, 41, 41, 40, 41, 41, 40, 38, 40, 41, 41, 41, 40, 41, 38])\t[('AS', 0), ('XN', 0), ('XM', 0), ('XO', 0), ('XG', 0), ('NM', 0), ('MD', '50'), ('YT', 'UU'), ('XS', '+'), ('NH', 6), ('CC', 'chr15'), ('CP', 102517980), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:2311:12870:78720\t256\t#0\t13141\t0\t50M\t*\t0\t0\tCTGAGGCTGAGGAAGGAGAAGGGGATGCACTGTTGGGGAGGCAGCTGTAA\tarray('B', [34, 34, 34, 37, 37, 37, 37, 37, 39, 39, 39, 39, 39, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 40, 40, 41, 41, 39, 40, 41, 41, 41, 41, 41, 41, 41, 41, 40, 41, 41, 41, 41, 41, 41, 41, 41, 40, 39, 39])\t[('AS', 0), ('XN', 0), ('XM', 0), ('XO', 0), ('XG', 0), ('NM', 0), ('MD', '50'), ('YT', 'UU'), ('XS', '+'), ('NH', 6), ('CC', 'chr15'), ('CP', 102517980), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:2214:15871:89780\t256\t#0\t13182\t0\t50M\t*\t0\t0\tCAGCTGTAACTCAAAGCCTTAGCCTCTGTTCCCACGAAGGCAGGGCCATC\tarray('B', [33, 33, 34, 37, 37, 37, 37, 37, 39, 39, 37, 37, 39, 41, 40, 40, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 38, 40, 41, 41, 38, 41, 41, 40, 40, 41, 41, 40, 40, 40, 41, 41, 41])\t[('AS', 0), ('XN', 0), ('XM', 0), ('XO', 0), ('XG', 0), ('NM', 0), ('MD', '50'), ('YT', 'UU'), ('XS', '+'), ('NH', 7), ('CC', 'chr12'), ('CP', 92382), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:1310:15939:43077\t256\t#0\t13760\t0\t50M\t*\t0\t0\tGGCCAGGCTTCTCACTGGGCCTCTGCAGGAGGCTGCCATTTGTCCTGCCC\tarray('B', [34, 34, 34, 37, 37, 37, 37, 37, 39, 39, 39, 39, 39, 40, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 40, 41, 40, 41, 40, 41, 41, 41])\t[('AS', 0), ('XN', 0), ('XM', 0), ('XO', 0), ('XG', 0), ('NM', 0), ('MD', '50'), ('YT', 'UU'), ('XS', '+'), ('NH', 6), ('CC', 'chr15'), ('CP', 102517361), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:1109:16844:5902\t16\t#0\t13826\t0\t50M\t*\t0\t0\tGACGGAGCAGACCCATCTGCTACTGCCCTTTCTATAATAACTAAAGTTAG\tarray('B', [40, 40, 40, 40, 40, 40, 40, 41, 40, 40, 39, 40, 40, 41, 40, 38, 38, 40, 41, 40, 38, 39, 38, 38, 41, 41, 41, 41, 41, 41, 40, 41, 41, 41, 40, 40, 40, 39, 37, 34, 39, 39, 37, 37, 37, 37, 37, 34, 34, 31])\t[('AS', 0), ('XN', 0), ('XM', 0), ('XO', 0), ('XG', 0), ('NM', 0), ('MD', '50'), ('YT', 'UU'), ('XS', '+'), ('NH', 6), ('CC', 'chr16'), ('CP', 63507), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:1313:8769:2379\t0\t#0\t13889\t3\t50M\t*\t0\t0\tTTCACCCCCTAGTCTCAATTTAAGAAGATCCCCATGGCCACAGGGCCCCT\tarray('B', [34, 34, 34, 37, 37, 37, 37, 37, 39, 39, 39, 39, 39, 40, 41, 41, 41, 41, 41, 41, 41, 40, 41, 41, 41, 41, 41, 40, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41])\t[('AS', 0), ('XN', 0), ('XM', 0), ('XO', 0), ('XG', 0), ('NM', 0), ('MD', '50'), ('YT', 'UU'), ('XS', '+'), ('NH', 2), ('CC', 'chr15'), ('CP', 102517232), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:2105:3662:61860\t256\t#0\t13889\t3\t50M\t*\t0\t0\tTTCACCCCCTAGTCTCAATTTAAGAAGATCCCCATGGCCACAGGGCCCCT\tarray('B', [34, 34, 34, 37, 37, 37, 37, 37, 39, 39, 39, 39, 39, 41, 40, 41, 41, 41, 41, 41, 41, 41, 41, 41, 40, 41, 41, 41, 41, 41, 38, 41, 41, 38, 39, 41, 41, 41, 41, 41, 41, 40, 41, 41, 41, 41, 41, 41, 41, 41])\t[('AS', 0), ('XN', 0), ('XM', 0), ('XO', 0), ('XG', 0), ('NM', 0), ('MD', '50'), ('YT', 'UU'), ('XS', '+'), ('NH', 2), ('CC', 'chr15'), ('CP', 102517232), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:2115:4415:58672\t256\t#0\t14003\t0\t50M\t*\t0\t0\tCTGCCCCACAGCCTTGCCTGGATTTGTATCTCCCTGGCTTGGTGCCAGTT\tarray('B', [33, 34, 31, 37, 37, 35, 37, 37, 39, 39, 37, 39, 39, 40, 41, 41, 38, 40, 40, 41, 40, 40, 41, 41, 41, 10, 31, 37, 39, 36, 39, 40, 41, 40, 41, 36, 38, 40, 40, 38, 40, 38, 39, 39, 40, 40, 41, 41, 39, 39])\t[('AS', -3), ('XN', 0), ('XM', 1), ('XO', 0), ('XG', 0), ('NM', 1), ('MD', '25C24'), ('YT', 'UU'), ('XS', '+'), ('NH', 6), ('CC', 'chr15'), ('CP', 102517118), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:1305:9390:78070\t272\t#0\t14065\t0\t50M\t*\t0\t0\tTGGCACCTCCCTCCCTCTCAACCACTTGAGCAAACTCCAAGACATCTTCT\tarray('B', [40, 41, 40, 39, 37, 41, 40, 40, 38, 41, 41, 41, 41, 41, 41, 41, 40, 38, 36, 39, 40, 40, 39, 39, 40, 41, 41, 41, 41, 41, 41, 41, 40, 40, 41, 40, 41, 38, 39, 39, 39, 39, 37, 37, 37, 37, 37, 34, 34, 31])\t[('AS', 0), ('XN', 0), ('XM', 0), ('XO', 0), ('XG', 0), ('NM', 0), ('MD', '50'), ('YT', 'UU'), ('XS', '+'), ('NH', 6), ('CC', 'chr15'), ('CP', 102517056), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:2216:19353:8868\t272\t#0\t14065\t0\t50M\t*\t0\t0\tTGGCACCTCCCTCCCTCTCAACCACTTGAGCAAACTCCAAGACATCTTCT\tarray('B', [37, 37, 27, 34, 33, 35, 37, 35, 31, 37, 27, 31, 35, 26, 36, 37, 33, 34, 31, 33, 34, 31, 31, 32, 36, 37, 37, 39, 32, 19, 32, 30, 36, 36, 34, 10, 29, 35, 19, 37, 32, 27, 35, 35, 35, 33, 32, 28, 28, 16])\t[('AS', 0), ('XN', 0), ('XM', 0), ('XO', 0), ('XG', 0), ('NM', 0), ('MD', '50'), ('YT', 'UU'), ('XS', '+'), ('NH', 6), ('CC', 'chr15'), ('CP', 102517056), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:2102:5196:24101\t272\t#0\t14067\t0\t50M\t*\t0\t0\tGCACCTCCCTCCCTCTCAACCACTTGAGCAAACTCCAAGACATCTTCTAC\tarray('B', [39, 37, 37, 41, 40, 39, 41, 41, 39, 36, 41, 41, 41, 40, 40, 39, 40, 39, 39, 41, 41, 41, 39, 37, 39, 40, 41, 41, 41, 41, 39, 37, 41, 39, 41, 41, 41, 39, 39, 39, 39, 35, 25, 37, 37, 37, 37, 34, 34, 33])\t[('AS', 0), ('XN', 0), ('XM', 0), ('XO', 0), ('XG', 0), ('NM', 0), ('MD', '50'), ('YT', 'UU'), ('XS', '+'), ('NH', 6), ('CC', 'chr15'), ('CP', 102517054), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:1102:15623:32994\t16\t#0\t14086\t0\t50M\t*\t0\t0\tCCACTTGAGCAAACTCCAAGACATCTTCTACCCCAACACCAGCAATTGTG\tarray('B', [40, 39, 40, 41, 41, 41, 41, 40, 39, 41, 40, 39, 40, 39, 38, 41, 41, 41, 41, 41, 41, 41, 40, 39, 40, 41, 41, 41, 41, 40, 41, 41, 41, 41, 41, 41, 40, 37, 39, 39, 39, 39, 37, 37, 37, 37, 37, 34, 33, 34])\t[('AS', 0), ('XN', 0), ('XM', 0), ('XO', 0), ('XG', 0), ('NM', 0), ('MD', '50'), ('YT', 'UU'), ('XS', '+'), ('NH', 6), ('CC', 'chr15'), ('CP', 102517035), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:2116:2095:30811\t272\t#0\t14086\t0\t50M\t*\t0\t0\tCCACTTGAGCAAACTCCAAGACATCTTCTACCCCAACACCAGCAATTGTG\tarray('B', [40, 40, 38, 41, 41, 41, 41, 40, 41, 41, 41, 41, 40, 39, 38, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 40, 39, 39, 37, 41, 41, 41, 41, 40, 40, 39, 37, 39, 39, 39, 39, 37, 37, 37, 37, 37, 34, 34, 34])\t[('AS', 0), ('XN', 0), ('XM', 0), ('XO', 0), ('XG', 0), ('NM', 0), ('MD', '50'), ('YT', 'UU'), ('XS', '+'), ('NH', 6), ('CC', 'chr15'), ('CP', 102517035), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:2205:2645:82154\t16\t#0\t14195\t0\t50M\t*\t0\t0\tACCTTTTTTGTGGGAGACTATTCCTCCCATCTGCAACAGCTGCCCCTGCT\tarray('B', [36, 41, 41, 38, 40, 40, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 40, 40, 41, 41, 41, 40, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 40, 41, 41, 41, 41, 39, 39, 39, 39, 39, 37, 37, 37, 37, 37, 34, 34, 34])\t[('AS', 0), ('XN', 0), ('XM', 0), ('XO', 0), ('XG', 0), ('NM', 0), ('MD', '50'), ('YT', 'UU'), ('XS', '+'), ('NH', 6), ('CC', 'chr15'), ('CP', 102516926), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:1208:17260:71677\t272\t#0\t14289\t0\t50M\t*\t0\t0\tCTGGGAGCTTCTGCCCCCACTGCCTAGGGACCAACAGGGGCAGGAGGCAG\tarray('B', [40, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 39, 39, 41, 40, 41, 41, 41, 41, 41, 40, 41, 40, 39, 41, 41, 41, 41, 41, 41, 41, 39, 39, 39, 39, 39, 37, 37, 37, 37, 37, 34, 34, 34])\t[('AS', 0), ('XN', 0), ('XM', 0), ('XO', 0), ('XG', 0), ('NM', 0), ('MD', '50'), ('YT', 'UU'), ('XS', '+'), ('NH', 7), ('CC', 'chr15'), ('CP', 102516832), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:2103:3746:80410\t272\t#0\t14289\t0\t50M\t*\t0\t0\tCTGGGAGCTTCTGCCCCCACTGCCTAGGGACCAACAGGGGCAGGAGGCAG\tarray('B', [41, 41, 41, 41, 41, 41, 40, 41, 41, 41, 41, 40, 40, 41, 41, 41, 41, 40, 38, 39, 40, 40, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 40, 41, 39, 39, 39, 39, 39, 37, 36, 36, 37, 37, 33, 34, 33])\t[('AS', 0), ('XN', 0), ('XM', 0), ('XO', 0), ('XG', 0), ('NM', 0), ('MD', '50'), ('YT', 'UU'), ('XS', '+'), ('NH', 7), ('CC', 'chr15'), ('CP', 102516832), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:2311:10014:5657\t272\t#0\t14289\t0\t50M\t*\t0\t0\tCTGGGAGCTTCTGCCCCCACTGCCTAGGGACCAACAGGGGCAGGAGGCAG\tarray('B', [41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 40, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 39, 39, 39, 39, 39, 37, 37, 37, 37, 37, 34, 34, 34])\t[('AS', 0), ('XN', 0), ('XM', 0), ('XO', 0), ('XG', 0), ('NM', 0), ('MD', '50'), ('YT', 'UU'), ('XS', '+'), ('NH', 7), ('CC', 'chr15'), ('CP', 102516832), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:1303:8918:82759\t256\t#0\t14399\t0\t50M\t*\t0\t0\tGTTGGTTTCTGCTCAGTTCTTTATTGATTGGTGTGCCGTTTTCTCTGGAA\tarray('B', [34, 34, 34, 37, 37, 37, 37, 37, 39, 39, 39, 39, 39, 41, 38, 40, 39, 39, 40, 41, 41, 41, 38, 40, 41, 40, 41, 41, 40, 39, 40, 39, 39, 38, 40, 41, 41, 41, 40, 41, 41, 41, 41, 41, 41, 41, 41, 41, 36, 38])\t[('AS', 0), ('XN', 0), ('XM', 0), ('XO', 0), ('XG', 0), ('NM', 0), ('MD', '50'), ('YT', 'UU'), ('XS', '-'), ('NH', 6), ('CC', 'chr16'), ('CP', 64080), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:2110:9307:89386\t0\t#0\t14403\t0\t50M\t*\t0\t0\tGTTTCTGCTCAGTTCTTTATTGATTGGTGTGCCGTTTTCTCTGGAAGCCT\tarray('B', [34, 34, 34, 37, 37, 37, 37, 37, 39, 39, 33, 39, 39, 39, 41, 41, 41, 41, 39, 41, 41, 41, 40, 41, 41, 40, 41, 39, 39, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41])\t[('AS', 0), ('XN', 0), ('XM', 0), ('XO', 0), ('XG', 0), ('NM', 0), ('MD', '50'), ('YT', 'UU'), ('XS', '-'), ('NH', 7), ('CC', 'chr15'), ('CP', 102516718), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:2307:13991:67891\t0\t#0\t14403\t0\t50M\t*\t0\t0\tGTTTCTGCTCAGTTCTTTATTGATTGGTGTGCCGTTTTCTCTGGAAGCCT\tarray('B', [34, 34, 34, 37, 37, 37, 37, 37, 39, 39, 39, 39, 39, 40, 41, 41, 39, 41, 38, 41, 41, 41, 40, 41, 41, 41, 41, 39, 40, 40, 40, 40, 40, 40, 38, 40, 41, 41, 40, 41, 40, 41, 41, 41, 41, 41, 41, 41, 41, 41])\t[('AS', 0), ('XN', 0), ('XM', 0), ('XO', 0), ('XG', 0), ('NM', 0), ('MD', '50'), ('YT', 'UU'), ('XS', '-'), ('NH', 7), ('CC', 'chr15'), ('CP', 102516718), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:2102:19752:11902\t256\t#0\t14404\t0\t50M\t*\t0\t0\tTTTCTGCTCAGTTCTTTATTGATTGGTGTGCCGTTTTCTCTGGAAGCCTC\tarray('B', [34, 34, 34, 37, 37, 37, 37, 37, 39, 39, 39, 39, 39, 41, 41, 41, 41, 41, 41, 41, 40, 41, 41, 41, 40, 41, 36, 37, 37, 39, 40, 41, 40, 38, 40, 40, 41, 41, 41, 41, 41, 41, 40, 41, 41, 41, 41, 41, 41, 41])\t[('AS', 0), ('XN', 0), ('XM', 0), ('XO', 0), ('XG', 0), ('NM', 0), ('MD', '50'), ('YT', 'UU'), ('XS', '-'), ('NH', 7), ('CC', 'chr15'), ('CP', 102516717), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:2306:16360:66747\t256\t#0\t14404\t0\t50M\t*\t0\t0\tTTTCTGCTCAGTTCTTTATTGATTGGTGTGCCGTTTTCTCTGGAAGCCTC\tarray('B', [34, 34, 34, 37, 37, 37, 37, 37, 39, 39, 39, 39, 39, 41, 41, 41, 41, 40, 41, 41, 41, 41, 41, 41, 41, 41, 39, 40, 40, 41, 40, 41, 40, 39, 40, 41, 41, 41, 41, 40, 41, 40, 41, 40, 41, 41, 41, 41, 40, 41])\t[('AS', 0), ('XN', 0), ('XM', 0), ('XO', 0), ('XG', 0), ('NM', 0), ('MD', '50'), ('YT', 'UU'), ('XS', '-'), ('NH', 7), ('CC', 'chr15'), ('CP', 102516717), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:2315:8343:31716\t256\t#0\t14404\t0\t50M\t*\t0\t0\tGTTCTGCTCAGTGCTTTATTGATTGGTGTGCCGTTTTCTCTGGAAGCCTC\tarray('B', [34, 34, 34, 37, 37, 37, 37, 37, 39, 39, 39, 39, 17, 27, 34, 38, 39, 39, 41, 41, 40, 40, 41, 41, 19, 37, 39, 16, 34, 38, 40, 41, 41, 40, 16, 34, 39, 40, 41, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\t[('AS', -8), ('XN', 0), ('XM', 2), ('XO', 0), ('XG', 0), ('NM', 2), ('MD', '0T11T37'), ('YT', 'UU'), ('XS', '-'), ('NH', 7), ('CC', 'chr15'), ('CP', 102516717), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:2303:13516:28835\t256\t#0\t14405\t0\t50M\t*\t0\t0\tGTCTGCTCAGTTCTTTATTGATTGGTGTGCCGTTTTCTCTGGAAGCCTCT\tarray('B', [34, 34, 34, 37, 37, 37, 37, 37, 38, 39, 39, 39, 39, 41, 40, 41, 37, 39, 41, 36, 40, 41, 41, 41, 41, 39, 39, 41, 40, 40, 41, 39, 41, 41, 41, 41, 41, 41, 40, 40, 40, 38, 40, 40, 40, 41, 41, 41, 41, 41])\t[('AS', -5), ('XN', 0), ('XM', 1), ('XO', 0), ('XG', 0), ('NM', 1), ('MD', '0T49'), ('YT', 'UU'), ('XS', '-'), ('NH', 7), ('CC', 'chr15'), ('CP', 102516716), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:2307:21404:55112\t256\t#0\t14436\t0\t50M\t*\t0\t0\tNTTTTCTCTGGAAGCCTCTTAAGAACACTGTGGCGCAGGCTGGGTGGAGC\tarray('B', [2, 16, 25, 30, 35, 37, 37, 37, 39, 39, 35, 39, 37, 38, 39, 38, 39, 40, 41, 41, 40, 41, 40, 39, 40, 40, 40, 40, 40, 35, 37, 39, 41, 41, 41, 41, 41, 41, 37, 40, 40, 38, 27, 37, 35, 33, 39, 38, 30, 36])\t[('AS', -7), ('XM', 2), ('XO', 0), ('XG', 0), ('MD', '0G27A21'), ('NM', 2), ('NH', 5), ('CC', 'chr12'), ('CP', 91140), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:2316:5136:15554\t256\t#0\t14436\t0\t50M\t*\t0\t0\tGTTTTCTCTGGAAGCCTCATAAGAACACTGTGGCGCAGGCTGGGTGGAGC\tarray('B', [31, 31, 31, 37, 37, 37, 37, 37, 39, 39, 39, 39, 38, 40, 40, 41, 41, 41, 10, 18, 32, 37, 39, 39, 41, 41, 38, 41, 41, 41, 39, 40, 41, 41, 41, 41, 40, 41, 40, 41, 40, 40, 40, 40, 33, 38, 39, 40, 40, 41])\t[('AS', -9), ('XM', 2), ('XO', 0), ('XG', 0), ('MD', '18T9A21'), ('NM', 2), ('NH', 5), ('CC', 'chr12'), ('CP', 91140), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:1102:18904:17515\t256\t#0\t14456\t0\t50M\t*\t0\t0\tAAGAACACAGTGGCGCAGGCTGGGTGGAGCCGTCCCCCCATGGAGCACAG\tarray('B', [31, 31, 31, 35, 35, 35, 35, 35, 39, 37, 39, 39, 39, 40, 32, 39, 40, 40, 40, 40, 38, 40, 40, 40, 25, 35, 38, 38, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 39, 39, 39, 39, 36, 36, 36, 36, 34, 34, 34])\t[('AS', 0), ('XN', 0), ('XM', 0), ('XO', 0), ('XG', 0), ('NM', 0), ('MD', '50'), ('YT', 'UU'), ('XS', '-'), ('NH', 6), ('CC', 'chr12'), ('CP', 91120), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:1110:17978:89994\t256\t#0\t14456\t0\t50M\t*\t0\t0\tAAGAACACAGTGGCGCAGGCTGGGTGGAGCCGTCCCCCCATGGAGCACAG\tarray('B', [23, 30, 31, 35, 37, 37, 37, 37, 37, 39, 35, 37, 39, 39, 40, 40, 40, 40, 40, 40, 38, 40, 40, 37, 24, 23, 35, 35, 38, 39, 38, 40, 39, 38, 36, 40, 40, 40, 40, 39, 39, 39, 38, 36, 35, 37, 28, 32, 31, 34])\t[('AS', 0), ('XN', 0), ('XM', 0), ('XO', 0), ('XG', 0), ('NM', 0), ('MD', '50'), ('YT', 'UU'), ('XS', '-'), ('NH', 6), ('CC', 'chr12'), ('CP', 91120), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:1204:15511:35912\t256\t#0\t14456\t0\t50M\t*\t0\t0\tAAGAACACAGTGGCGCAGGCTGGGTGGAGCCGTCCCCCCATGGAGCACAG\tarray('B', [31, 31, 31, 35, 35, 37, 37, 37, 38, 39, 27, 37, 39, 39, 39, 38, 39, 41, 40, 41, 41, 40, 40, 40, 38, 38, 38, 38, 40, 40, 38, 38, 40, 38, 39, 39, 39, 38, 38, 39, 37, 39, 38, 34, 37, 37, 35, 36, 34, 34])\t[('AS', 0), ('XN', 0), ('XM', 0), ('XO', 0), ('XG', 0), ('NM', 0), ('MD', '50'), ('YT', 'UU'), ('XS', '-'), ('NH', 6), ('CC', 'chr12'), ('CP', 91120), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:1303:20278:71654\t256\t#0\t14456\t0\t50M\t*\t0\t0\tAAGAACACAGTGGCGCAGGCTGGGTGGAGCCGTCCCCCCATGGAGCACAG\tarray('B', [23, 30, 28, 35, 35, 35, 33, 30, 35, 39, 32, 34, 35, 37, 39, 32, 38, 36, 38, 38, 38, 40, 38, 39, 30, 34, 37, 38, 28, 37, 37, 35, 37, 39, 40, 38, 34, 27, 32, 36, 39, 36, 37, 35, 30, 35, 33, 36, 34, 34])\t[('AS', 0), ('XN', 0), ('XM', 0), ('XO', 0), ('XG', 0), ('NM', 0), ('MD', '50'), ('YT', 'UU'), ('XS', '-'), ('NH', 6), ('CC', 'chr12'), ('CP', 91120), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:1311:18825:81494\t0\t#0\t14456\t0\t50M\t*\t0\t0\tAAGAACACAGTGGCGCAGGCTGGGTGGAGCCGTCCCCCCATGGAGCACAG\tarray('B', [34, 34, 34, 37, 37, 37, 37, 37, 39, 39, 39, 39, 39, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 38, 39, 40, 38, 40, 40, 40, 41, 41, 40, 41, 40, 41, 41, 41, 39, 39, 39, 39, 35, 36, 37, 35, 36, 36, 36])\t[('AS', 0), ('XN', 0), ('XM', 0), ('XO', 0), ('XG', 0), ('NM', 0), ('MD', '50'), ('YT', 'UU'), ('XS', '-'), ('NH', 6), ('CC', 'chr12'), ('CP', 91120), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:1214:19404:45466\t16\t#0\t14456\t0\t50M\t*\t0\t0\tAAGAACACAGTGGCGCAGGCTGGGTGGAGCCGTCCCCCCATGGAGCACAG\tarray('B', [35, 36, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 41, 41, 41, 39, 39, 39, 39, 39, 37, 37, 37, 37, 37, 34, 34, 34])\t[('AS', 0), ('XN', 0), ('XM', 0), ('XO', 0), ('XG', 0), ('NM', 0), ('MD', '50'), ('YT', 'UU'), ('XS', '-'), ('NH', 6), ('CC', 'chr12'), ('CP', 91120), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:2101:15111:50951\t272\t#0\t14456\t0\t50M\t*\t0\t0\tAAGAACACAGTGGCGCAGGCTGGGTGGAGCCGTCCCCCCATGGAGCACAG\tarray('B', [35, 36, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 41, 41, 41, 39, 39, 39, 39, 39, 37, 37, 37, 37, 37, 34, 34, 34])\t[('AS', 0), ('XN', 0), ('XM', 0), ('XO', 0), ('XG', 0), ('NM', 0), ('MD', '50'), ('YT', 'UU'), ('XS', '-'), ('NH', 6), ('CC', 'chr12'), ('CP', 91120), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:2113:14319:36837\t272\t#0\t14456\t0\t50M\t*\t0\t0\tAAGAACACAGTGGCGCAGGCTGGGTGGAGCCGTCCCCCCATGGAGCACAG\tarray('B', [35, 36, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 41, 41, 41, 39, 39, 39, 39, 39, 37, 37, 37, 37, 37, 34, 34, 33])\t[('AS', 0), ('XN', 0), ('XM', 0), ('XO', 0), ('XG', 0), ('NM', 0), ('MD', '50'), ('YT', 'UU'), ('XS', '-'), ('NH', 6), ('CC', 'chr12'), ('CP', 91120), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:2113:21230:77027\t16\t#0\t14456\t0\t50M\t*\t0\t0\tAAGAACACAGTGGCGCAGGCTGGGTGGAGCCGTCCCCCCATGGAGCACAG\tarray('B', [35, 36, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 41, 41, 41, 39, 39, 39, 39, 39, 37, 37, 37, 37, 37, 34, 34, 34])\t[('AS', 0), ('XN', 0), ('XM', 0), ('XO', 0), ('XG', 0), ('NM', 0), ('MD', '50'), ('YT', 'UU'), ('XS', '-'), ('NH', 6), ('CC', 'chr12'), ('CP', 91120), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:1311:4412:9009\t256\t#0\t14457\t0\t50M\t*\t0\t0\tAGAACACAGTGGCGCAGGCTGGGTGGAGCCGTCCCCCCATGGAGCACAGG\tarray('B', [34, 34, 34, 37, 37, 37, 37, 37, 39, 39, 39, 39, 39, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 37, 39, 40, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 39, 39, 39, 39, 39, 37, 37, 37, 37, 36, 36, 35])\t[('AS', 0), ('XN', 0), ('XM', 0), ('XO', 0), ('XG', 0), ('NM', 0), ('MD', '50'), ('YT', 'UU'), ('XS', '-'), ('NH', 6), ('CC', 'chr12'), ('CP', 91119), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:2212:17817:6567\t256\t#0\t14457\t0\t50M\t*\t0\t0\tAGAACACAGTGGCGCAGGCTGGGTGGAGCCGTCCCCCCATGGAGCACAGG\tarray('B', [34, 34, 34, 37, 37, 37, 37, 37, 39, 39, 39, 39, 39, 41, 41, 40, 41, 41, 41, 41, 41, 41, 41, 37, 39, 40, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 39, 39, 39, 39, 39, 37, 37, 37, 37, 36, 36, 35])\t[('AS', 0), ('XN', 0), ('XM', 0), ('XO', 0), ('XG', 0), ('NM', 0), ('MD', '50'), ('YT', 'UU'), ('XS', '-'), ('NH', 6), ('CC', 'chr12'), ('CP', 91119), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:2212:3100:34477\t0\t#0\t14457\t0\t50M\t*\t0\t0\tAGAACACAGTGGCGCAGGCTGGGTGGAGCCGTCCCCCCATGGAGCACAGG\tarray('B', [34, 34, 34, 37, 37, 37, 37, 37, 39, 39, 39, 39, 39, 41, 40, 41, 41, 41, 41, 41, 41, 41, 41, 30, 37, 39, 40, 40, 34, 38, 40, 41, 41, 41, 41, 41, 41, 41, 39, 39, 39, 39, 39, 37, 37, 37, 37, 36, 36, 35])\t[('AS', 0), ('XN', 0), ('XM', 0), ('XO', 0), ('XG', 0), ('NM', 0), ('MD', '50'), ('YT', 'UU'), ('XS', '-'), ('NH', 6), ('CC', 'chr12'), ('CP', 91119), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:1101:18350:61836\t272\t#0\t14457\t0\t50M\t*\t0\t0\tAGAACACAGTGGCGCAGGCTGGGTGGAGCCGTCCCCCCATGGAGCACAGG\tarray('B', [36, 35, 35, 35, 35, 34, 35, 35, 35, 31, 35, 35, 35, 35, 35, 35, 35, 35, 33, 31, 35, 35, 34, 31, 35, 35, 35, 35, 35, 35, 35, 33, 35, 41, 41, 41, 40, 39, 37, 30, 39, 39, 37, 37, 37, 37, 37, 34, 34, 34])\t[('AS', 0), ('XN', 0), ('XM', 0), ('XO', 0), ('XG', 0), ('NM', 0), ('MD', '50'), ('YT', 'UU'), ('XS', '-'), ('NH', 6), ('CC', 'chr12'), ('CP', 91119), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:1105:13916:43101\t272\t#0\t14457\t0\t50M\t*\t0\t0\tAGAACACAGTGGCGCAGGCTGGGTGGAGCCGTCCCCCCATGGAGCACAGG\tarray('B', [36, 35, 35, 35, 35, 34, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 41, 41, 41, 41, 39, 39, 39, 39, 39, 37, 37, 37, 37, 37, 34, 34, 34])\t[('AS', 0), ('XN', 0), ('XM', 0), ('XO', 0), ('XG', 0), ('NM', 0), ('MD', '50'), ('YT', 'UU'), ('XS', '-'), ('NH', 6), ('CC', 'chr12'), ('CP', 91119), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:1215:15386:85165\t272\t#0\t14458\t0\t50M\t*\t0\t0\tGAACACTGTGGCGCAGGCTGGGTGGAGCCTTCCCCCCATGGAGCACAGGC\tarray('B', [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 31, 28, 31, 30, 27, 26, 31, 27, 29, 26, 25, 27, 26, 20])\t[('AS', -4), ('XN', 0), ('XM', 2), ('XO', 0), ('XG', 0), ('NM', 2), ('MD', '6A22G20'), ('YT', 'UU'), ('XS', '-'), ('NH', 6), ('CC', 'chr12'), ('CP', 91118), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:1303:14824:51697\t272\t#0\t14458\t0\t50M\t*\t0\t0\tGAACACTGTGGCGCAGGCTGGGTGGAGCTGTCCCCCCATGGAGCACAGGC\tarray('B', [35, 36, 36, 35, 35, 35, 35, 35, 33, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 33, 27, 7, 35, 35, 35, 40, 40, 40, 40, 40, 39, 35, 39, 39, 39, 37, 37, 37, 37, 37, 34, 34, 34])\t[('AS', -7), ('XN', 0), ('XM', 2), ('XO', 0), ('XG', 0), ('NM', 2), ('MD', '6A21C21'), ('YT', 'UU'), ('XS', '-'), ('NH', 6), ('CC', 'chr12'), ('CP', 91118), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:2202:16637:75379\t272\t#0\t14458\t0\t50M\t*\t0\t0\tGAACACTGTGGCGCAGGCTGGGTGGAGCCGTCCCCCCATGGAGCACAGGC\tarray('B', [35, 36, 36, 35, 35, 35, 33, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 41, 41, 41, 40, 39, 37, 29, 39, 39, 39, 37, 37, 37, 37, 37, 34, 34, 34])\t[('AS', -5), ('XN', 0), ('XM', 1), ('XO', 0), ('XG', 0), ('NM', 1), ('MD', '6A43'), ('YT', 'UU'), ('XS', '-'), ('NH', 6), ('CC', 'chr12'), ('CP', 91118), ('HI', 0)]\n",
      "HS1:329:C3EGUACXX:5:2206:2139:37526\t272\t#0\t14458\t0\t50M\t*\t0\t0\tGAACACAGTGGCGCAGGCTGGGTGGAGCCGTCCCCCCATGGAGCACAGGC\tarray('B', [35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 33, 35, 41, 41, 41, 41, 41, 39, 39, 39, 39, 39, 37, 37, 37, 37, 37, 34, 34, 34])\t[('AS', 0), ('XN', 0), ('XM', 0), ('XO', 0), ('XG', 0), ('NM', 0), ('MD', '50'), ('YT', 'UU'), ('XS', '-'), ('NH', 6), ('CC', 'chr12'), ('CP', 91118), ('HI', 0)]\n"
     ]
    }
   ],
   "source": [
    "save = pysam.set_verbosity(0)\n",
    "samfile = pysam.AlignmentFile(\"/Users/sergiomares/Downloads/293T1_sorted.bam\", \"rb\")\n",
    "a = 0\n",
    "b = []\n",
    "\n",
    "\n",
    "for read in samfile:\n",
    "    if a != 50:\n",
    "        print(read)\n",
    "        a += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d25f43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len('CCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCAACACA')\n",
    "\n",
    "counts = [34, 34, 34, 37, 37, 37, 37, 37, 39, 39, 38, 39, 39, 41, 41, 40, 41, 40, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 40, 40, 40, 40, 41, 41, 41, 41, 41, 40, 41, 41, 41, 41, 41, 40, 40, 41, 41, 41, 41, 41]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

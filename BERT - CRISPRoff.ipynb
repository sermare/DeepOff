{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "733f8cec",
   "metadata": {},
   "source": [
    "# BERT - CRISPRoff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ea3563",
   "metadata": {},
   "source": [
    "Tutorial adapted from https://colab.research.google.com/github/antonio-f/BERT_from_scratch/blob/main/BERT_from_scratch.ipynb#scrollTo=qwf2L-wZFJo0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c63fa6",
   "metadata": {},
   "source": [
    "### 0. Useful links\n",
    "\n",
    "https://datascience.stackexchange.com/questions/54412/how-to-add-a-cnn-layer-on-top-of-bert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165d95b9",
   "metadata": {},
   "source": [
    "### 1. Packages and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af4bedb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "n_sequences = 10\n",
    "kmer_length = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "461719fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from subprocess import PIPE, run\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import pprint\n",
    "import requests\n",
    "\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "Data = pd.read_csv('/Users/sergiomares/Desktop/Nunez/Jin file/TSS_CpG_crispriphenotype_table.txt', delimiter = '\\t',header = 0)\n",
    "\n",
    "b = []\n",
    "\n",
    "for i in range (n_sequences):\n",
    "    datas = requests.get('http://togows.org/api/ucsc/hg19/'+ str(Data['chromosome'][i])+':'+str(int(Data[\"Primary TSS, 5'\"][i]-1000))+'-'+str(Data[\"Primary TSS, 5'\"][i]+1000)).text.replace('\\n','')    \n",
    "    b.append(datas)\n",
    "\n",
    "Promoter_sequences = pd.DataFrame(b)\n",
    "\n",
    "def getKmers(sequence, size=6):\n",
    "    return [sequence[x:x+size].lower() for x in range(len(sequence) - size + 1)]\n",
    "\n",
    "Promoter_sequences = Promoter_sequences.apply(lambda x: getKmers(x[0], kmer_length), axis = 1)\n",
    "Promoter_sequences = pd.DataFrame(Promoter_sequences)\n",
    "\n",
    "Promoters_text = list(Promoter_sequences[0])\n",
    "\n",
    "for item in range(len(Promoter_sequences)):\n",
    "    Promoters_text[item] = ' '.join(Promoter_sequences[0][item])\n",
    "\n",
    "y_data = Promoter_sequences.iloc[:, 0].values     \n",
    "\n",
    "Promoter_sequences[1] = Data.index[:(len(Promoter_sequences))]\n",
    "Promoter_sequences.columns = [['sequence', 'index']]\n",
    "Promoter_sequences['text'] = pd.DataFrame(Promoters_text)\n",
    "\n",
    "#dataset = dict(pd.MultiIndex.from_frame(Promoter_sequences[['index','text']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e6988a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: [\"('index',)\", \"('text',)\"],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "datasets = Dataset.from_pandas(Promoter_sequences[['index','text']])\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47b13e4",
   "metadata": {},
   "source": [
    "### 2. Building a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "702e6d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_text(dataset, output_filename=\"data.txt\"):\n",
    "  \"\"\"Utility function to save dataset text to disk,\n",
    "  useful for using the texts to train the tokenizer \n",
    "  (as the tokenizer accepts files)\"\"\"\n",
    "  with open(output_filename, \"w\") as f:\n",
    "    for t in dataset:\n",
    "      print(t, file=f)\n",
    "\n",
    "dataset_to_text(datasets, \"train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9e468dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "tokenizer.train(files='train.txt', \n",
    "                vocab_size=30_522,\n",
    "                min_frequency=6,\n",
    "                special_tokens=['<s>', '<pad>', '</s>', '<unk>', '<mask>']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c91363ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['liberto/vocab.json', 'liberto/merges.txt']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "os.mkdir('./liberto')\n",
    "tokenizer.save_model('liberto')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce4537e",
   "metadata": {},
   "source": [
    "### 3. Initializing the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "22252915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('liberto',max_length = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "05729852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 277, 277, 2]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer('gcgc')\n",
    "tokens.input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabf4bc8",
   "metadata": {},
   "source": [
    "### 4. Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4beeb72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tokenizer( Promoters_text , max_length=10, padding='max_length', truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "72d38609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "labels = torch.tensor([x for x in batch['input_ids']])\n",
    "mask = torch.tensor([x for x in batch['attention_mask']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c0c781f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,   88,  284,  261,  580,  584,  543,  861,  341,    2],\n",
       "        [   0,  271,  305,  613,  541,  790,  414,  521,  432,    2],\n",
       "        [   0,  262,  353,  431,  736,  761, 1220, 1110, 1217,    2],\n",
       "        [   0,   88,   69,  312,  802, 1076,  866,  684,  512,    2],\n",
       "        [   0,  284,  283,  422,  382,  392,  783,  492,  617,    2],\n",
       "        [   0,  286,  271,  925, 1122,  511,  529,  526,  688,    2],\n",
       "        [   0,   88,   71,  351,  427,  325,  288,  388, 1250,    2],\n",
       "        [   0,  262,  285,  444,  494,  514,  493,  557,  426,    2],\n",
       "        [   0,  287,  261, 1012,  681,  622,  517,  621, 1263,    2],\n",
       "        [   0,  319,  262, 1286, 1030,  450,  892,  608,  537,    2]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c9bc5b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make copy of labels tensor, this will be input_ids\n",
    "input_ids = labels.detach().clone()\n",
    "# create random array of floats with equal dims to input_ids\n",
    "rand = torch.rand(input_ids.shape)\n",
    "# mask random 15% where token is not 0 [PAD], 1 [CLS], or 2 [SEP]\n",
    "# mask_arr = (rand < .15) * (input_ids != 0) * (input_ids != 1) * (input_ids != 2)\n",
    "mask_arr = (rand < .15) * (input_ids > 2) \n",
    "# loop through each row in input_ids tensor (cannot do in parallel)\n",
    "for i in range(input_ids.shape[0]):\n",
    "    # get indices of mask positions from mask array\n",
    "    selection = torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "    # mask input_ids\n",
    "    input_ids[i, selection] = 3  # our custom [MASK] token == 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9391248b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4b0dd14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = {'input_ids': input_ids, 'attention_mask': mask, 'labels': labels} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "da590a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        # store encodings internally\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        # return the number of samples\n",
    "        return self.encodings['input_ids'].shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # return dictionary of input_ids, attention_mask, and labels for index i\n",
    "        return {key: tensor[i] for key, tensor in self.encodings.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "306f36a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(encodings) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "183f6567",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562f998e",
   "metadata": {},
   "source": [
    "### 5. Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5f169e",
   "metadata": {},
   "source": [
    "#### A. Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6549e4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=30_522,  # we align this to the tokenizer vocab_size\n",
    "    max_position_embeddings=514,\n",
    "    hidden_size=768,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ed44a656",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "model = RobertaForMaskedLM(config) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4011086c",
   "metadata": {},
   "source": [
    "#### B. Training Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ecfcfc7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForMaskedLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# and move our model over to the selected device\n",
    "model.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3cb2561e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergiomares/miniconda3/envs/tf/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# activate training mode\n",
    "model.train()\n",
    "# initialize optimizer\n",
    "optim = AdamW(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ec0058",
   "metadata": {},
   "source": [
    "#### C. Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f30e72b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:01<00:00,  1.29s/it, loss=10.5]\n",
      "Epoch 1: 100%|██████████| 1/1 [00:01<00:00,  1.61s/it, loss=8.85]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # setup loop with TQDM and dataloader\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    for batch in loop:\n",
    "        # initialize calculated gradients (from prev step)\n",
    "        optim.zero_grad()\n",
    "        # pull all tensor batches required for training\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        # process\n",
    "        outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                        labels=labels)\n",
    "        # extract loss\n",
    "        loss = outputs.loss\n",
    "        # calculate loss for every parameter that needs grad update\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "        # print relevant info to progress bar\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ef9e1f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./liberto')  # and don't forget to save liBERTo!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d60e255",
   "metadata": {},
   "source": [
    "### 6. The real test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ec6c1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "07630e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates file too large for github to sync\n",
    "#fill = pipeline('fill-mask', model='liberto', tokenizer='liberto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1de63cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.0016671427292749286,\n",
       "  'token': 0,\n",
       "  'token_str': '<s>',\n",
       "  'sequence': 'gcgc tata '},\n",
       " {'score': 0.00134976115077734,\n",
       "  'token': 2,\n",
       "  'token_str': '</s>',\n",
       "  'sequence': 'gcgc tata '},\n",
       " {'score': 0.00031376106198877096,\n",
       "  'token': 262,\n",
       "  'token_str': 'aa',\n",
       "  'sequence': 'gcgcaa tata '},\n",
       " {'score': 0.00026194300153292716,\n",
       "  'token': 15426,\n",
       "  'token_str': '',\n",
       "  'sequence': 'gcgc tata '},\n",
       " {'score': 0.00023920593957882375,\n",
       "  'token': 11941,\n",
       "  'token_str': '',\n",
       "  'sequence': 'gcgc tata '}]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill(f'gcgc {fill.tokenizer.mask_token} tata ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7ca7fe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do: Fork ENFORMER from Deepmind repo and adapt it to include methylation information"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

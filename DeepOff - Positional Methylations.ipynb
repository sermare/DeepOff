{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a622e70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from subprocess import PIPE, run\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import pprint\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce3e836",
   "metadata": {},
   "source": [
    "#### Data Bullshit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a5d8080",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.read_csv('/Users/sergiomares/Desktop/Nunez/Jin file/TSS_CpG_crispriphenotype_table.txt', delimiter = '\\t',header = 0)\n",
    "Data = Data.sort_values([\"gene\",'average phenotype of strongest 3'], ascending = True).drop_duplicates(subset = 'gene', keep = 'first')\n",
    "UCSC_TSS = pd.read_csv('UCSC_TSS.txt', delimiter = '\\t',header = 0)\n",
    "UCSC_TSS = UCSC_TSS.drop_duplicates() \n",
    "\n",
    "errors = []\n",
    "\n",
    "for gene in (Data['gene']):\n",
    "        try:\n",
    "                if len(UCSC_TSS[UCSC_TSS['hg19.kgXref.geneSymbol'] == gene][\"hg19.knownGene.txStart\"]) > 1:\n",
    "                        x = abs(UCSC_TSS[UCSC_TSS['hg19.kgXref.geneSymbol'] == gene][\"hg19.knownGene.txStart\"] - Data[Data['gene'] == gene][\"Primary TSS, 3'\"].iloc[-1]).min()\n",
    "                        Data.loc[Data['gene'] == gene, \"Primary TSS, 3'\"] = (int(Data[Data['gene'] == gene][\"Primary TSS, 3'\"].iloc[-1] - int(x) + 1))\n",
    "                else:\n",
    "                        x = UCSC_TSS[UCSC_TSS['hg19.kgXref.geneSymbol'] == gene][\"hg19.knownGene.txStart\"].iloc[-1]\n",
    "                        Data.loc[Data['gene'] == gene, \"Primary TSS, 3'\"] = int(x) + 1 \n",
    "              \n",
    "\n",
    "        except:\n",
    "            errors.append(gene)\n",
    "\n",
    "len(errors)\n",
    "\n",
    "Data = Data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a14d6f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "Promoter_sequences = pd.read_csv('3kb_Promoter.sequences-2.csv', sep=',', header = 0)\n",
    "Promoter_sequences = pd.merge(how = 'outer', left = Promoter_sequences, right = Data, left_on = 'Gene', right_on = 'gene')\n",
    "Promoter_sequences = Promoter_sequences.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f406a5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpgs = pd.read_csv('1-s2-S0092867421003536-mmc3.csv',sep = ',',  header = 0)\n",
    "cpgs = pd.DataFrame(cpgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b1dcda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(how = 'outer', left = Promoter_sequences, right = cpgs, left_on = 'Gene', right_on = 'gene')\n",
    "df = df.loc[df.Gene.notna()]\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c4f9f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "irbs = pd.read_csv('wgEncodeHaibMethylRrbsK562HaibSitesRep1.bed', sep='\\t', header = 0)\n",
    "#Extract only the values with 100% Certainty of methylation \n",
    "irbs = irbs[irbs['Unnamed: 8'] == '255,0,0']\n",
    "\n",
    "chromosome_list = np.unique(irbs.track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5acf5522",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.zeros((len(df),3001))\n",
    "\n",
    "for index, chromosomes in enumerate(chromosome_list):\n",
    "    \n",
    "    table = irbs[irbs['track'] == chromosomes]\n",
    "    table2 = df[df.chromosome == chromosomes]\n",
    "\n",
    "    for i, x in enumerate(table['name=\"SL725.1']):\n",
    "\n",
    "\n",
    "        for o, z in enumerate(table2[\"Primary TSS, 3'\"]):\n",
    "\n",
    "            if (x > (z - 1500) and (x < (z + 1500))) == True:\n",
    "                y = x - (z - 1500)\n",
    "                #print((z - 1500), z,  (z + 1500))\n",
    "                #print('CpG Islands found in:', x, 'Position on vector:', y,\"in row\", Promoter_sequences['gene'][o], o + 1)    \n",
    "\n",
    "                tmp[df.level_0[df[\"Primary TSS, 3'\"] == z ].iloc[-1] + 1][int(y)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d9c6fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to show each position for a certain gene\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#### Nilah recommended creating a plot for methylation near TSS (position and its phenotype score)\n",
    "\n",
    "def methylated_basepairs(int):\n",
    "    if len(np.where(tmp[int] == 1)[0]) > 0:\n",
    "        is_c(int)\n",
    "        is_c(int)\n",
    "\n",
    "        x = df.head(int).tail(1)['Gene'].iloc[-1]\n",
    "        # print(x, \"| Coordinates of TSS:\", df[df['Gene'] == x][\"Primary TSS, 3'\"].iloc[-1])\n",
    "        # print(\"Found\", len([np.where(tmp[int] == 1)][0][0]) , \"Methylated basepairs at positions:\")\n",
    "        # print([np.where(tmp[int] == 1)][0][0] + df[df['Gene'] == x][\"Primary TSS, 3'\"].iloc[-1] - 1500)\n",
    "        # plt.title(\"Methylation profile for promoter region of gene: {}\".format(x), size = 15)\n",
    "        # plt.xlabel('Position of the bases | 1500 = Start of TSS ')\n",
    "        # plt.ylabel('Methylation')\n",
    "        # plt.plot(tmp[int])\n",
    "\n",
    "        # #print(df[df['Gene'] == x][\"chromosome\"].iloc[-1],':',[np.where(tmp[int] == 1)][0][0][0] + df[df['Gene'] == x][\"Primary TSS, 3'\"].iloc[-1] - 1500, '-', [np.where(tmp[int] == 1)][0][0][0] + df[df['Gene'] == x][\"Primary TSS, 3'\"].iloc[-1] - 1500 + 1)\n",
    "        # print(df[df['Gene'] == x][\"chromosome\"].iloc[-1],':', df[df['Gene'] == x][\"Primary TSS, 3'\"].iloc[-1] - 1500, '-', df[df['Gene'] == x][\"Primary TSS, 3'\"].iloc[-1] + 1500 )\n",
    "\n",
    "        #return([np.where(tmp[int] == 1)][0][0])\n",
    "\n",
    "        return(len([np.where(tmp[int] == 1)][0][0] + df[df['Gene'] == x][\"Primary TSS, 3'\"].iloc[-1]))\n",
    "\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Are the predicted methylation sites C?  \n",
    "\n",
    "def is_c(int):\n",
    "    \n",
    "    counter = 0\n",
    "    total = 0\n",
    "\n",
    "    try:\n",
    "        x = [np.where(tmp[int] == 1)][0][0]\n",
    "        y = df.head(int).tail(1)['Gene'].iloc[-1]\n",
    "\n",
    "        if len(x) == 0:\n",
    "            #print(\"No Methylated Cs\")\n",
    "            counter = 2\n",
    "            \n",
    "        for i in x:\n",
    "\n",
    "            total += 1\n",
    "\n",
    "            if list(df.Sequences[df['Gene'] == y].iloc[-1])[i] == 'C':\n",
    "                #print(i, 'is a C')\n",
    "                counter +=1 \n",
    "            else:\n",
    "                #print(i, 'is not a C, its a ' + list(df.Sequences[df['Gene'] == y].iloc[-1])[i])\n",
    "                tmp[int][i] = 0\n",
    "    except:\n",
    "        print(\"Issue with int\", int, \"Gene:\")\n",
    "\n",
    "    #print('Correct Cs:',counter/total*100,'%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb9bf560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 13,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 30,\n",
       " 8,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 14,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 14,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 28,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 24,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 12,\n",
       " 0,\n",
       " 6,\n",
       " 7,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 10,\n",
       " 0,\n",
       " 0,\n",
       " 12,\n",
       " 12,\n",
       " 21,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 14,\n",
       " 0,\n",
       " 13,\n",
       " 9,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 50,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 17,\n",
       " 7,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 38,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 12,\n",
       " 1,\n",
       " 15,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 32,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 22,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 17,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 17,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 1,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 17,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 27,\n",
       " 1,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 16,\n",
       " 25,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 7,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 12,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 7,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 26,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 13,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "methylated_cs = [methylated_basepairs(i) for i in range(20340)]\n",
    "methylated_cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4f06e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20340\n",
      "2079\n",
      "3491\n",
      "4014\n",
      "5695\n",
      "5971\n",
      "6588\n",
      "7741\n",
      "9579\n",
      "9831\n",
      "10451\n",
      "11164\n",
      "19212\n",
      "error with 20328\n",
      "error with 20329\n",
      "error with 20330\n",
      "error with 20331\n",
      "error with 20332\n",
      "error with 20333\n",
      "error with 20334\n",
      "error with 20335\n",
      "error with 20336\n",
      "error with 20337\n",
      "error with 20338\n",
      "error with 20339\n",
      "20328\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "sequences = df.Sequences\n",
    "\n",
    " # This removes empty sequences.\n",
    "\n",
    "# The LabelEncoder encodes a sequence of bases as a sequence of integers.\n",
    "integer_encoder = LabelEncoder()  \n",
    "# The OneHotEncoder converts an array of integers to a sparse matrix where \n",
    "# each row corresponds to one possible value of each feature.\n",
    "one_hot_encoder = OneHotEncoder(categories='auto')   \n",
    "input_features = []\n",
    "\n",
    "counter = 0 \n",
    "for sequence in sequences:\n",
    "    try:\n",
    "        integer_encoded = integer_encoder.fit_transform(list(sequence))\n",
    "        integer_encoded = np.array(integer_encoded).reshape(-1, 1)\n",
    "        one_hot_encoded = one_hot_encoder.fit_transform(integer_encoded)\n",
    "        input_features.append(one_hot_encoded.toarray())\n",
    "    except:\n",
    "        print('error with', sequence)\n",
    "\n",
    "indexes = []\n",
    "\n",
    "print(len(input_features))\n",
    "\n",
    "for i in range(len(input_features)):\n",
    "    try:\n",
    "        if input_features[i].shape != (3001, 4):\n",
    "            indexes.append(i)\n",
    "            input_features.pop(i)\n",
    "            print(i)\n",
    "    except:\n",
    "        print('error with', i)    \n",
    "print(len(input_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "268db969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sequence\n",
      "-----------------------\n",
      "DNA Sequence #1:\n",
      " CAAGGAAAAC ... AGCCAAGATT\n",
      "One hot encoding of Sequence #1:\n",
      " [[0. 1. 1. ... 1. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(threshold=40)\n",
    "input_features = np.stack(input_features)\n",
    "print(\"Example sequence\\n-----------------------\")\n",
    "print('DNA Sequence #1:\\n',sequences[0][:10],'...',sequences[0][-10:])\n",
    "print('One hot encoding of Sequence #1:\\n',input_features[0].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "890ee9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20340\n",
      "20328\n"
     ]
    }
   ],
   "source": [
    "print(len(tmp))\n",
    "tmp_1 = np.delete(tmp, indexes, axis = 0)\n",
    "print(len(tmp_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b7d2ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20328, 3001)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3625aaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_input_features = np.dstack((input_features, tmp_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87ebc317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20340\n",
      "20328\n"
     ]
    }
   ],
   "source": [
    "print(len(df['CRISPRoff_average']))\n",
    "df = df.drop(index = indexes)\n",
    "print(len(df['CRISPRoff_average']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db760f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras import backend as K\n",
    "# K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1caa8634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features_1, test_features, train_labels, test_labels = train_test_split(\n",
    "    input_features, df['CRISPRoff_average'], test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99392c0b",
   "metadata": {},
   "source": [
    "## Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "233da678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "1b3eb91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sequence\n",
      "-----------------------\n",
      "DNA Sequence #1:\n",
      " CCGAGGGCTA ... CGCGGACACC\n",
      "Classes:  ['A' 'C' 'G' 'T']\n",
      "One hot encoding of Sequence #1:\n",
      " [[0. 1. 1. ... 1. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Labels:\n",
      " [['0' '0' '0' ... '0' '1' '1']]\n",
      "One-hot encoded labels:\n",
      " 0       -0.063267\n",
      "1        0.029424\n",
      "2        0.021009\n",
      "3        0.012236\n",
      "4        0.006076\n",
      "           ...   \n",
      "20335    0.020289\n",
      "20336    0.012916\n",
      "20337    0.004499\n",
      "20338    0.029630\n",
      "20339   -0.242959\n",
      "Name: CRISPRoff_average, Length: 20328, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_features = np.stack(md_input_features)\n",
    "print(\"Example sequence\\n-----------------------\")\n",
    "print('DNA Sequence #1:\\n',sequences[0][:10],'...',sequences[0][-10:])\n",
    "print('Classes: ', integer_encoder.classes_)\n",
    "print('One hot encoding of Sequence #1:\\n',input_features[0].T)\n",
    "input_features.shap\n",
    "\n",
    "input_labels = df['CRISPRoff_average']\n",
    "\n",
    "print('Labels:\\n',labels.T)\n",
    "print('One-hot encoded labels:\\n',input_labels.T)\n",
    "\n",
    "\n",
    "# %% Split data\n",
    "input_features = input_features.transpose(0,2,1).astype(np.float32) # adapt to pytorch input format [N, C, L]\n",
    "input_features.shape\n",
    "input_labels = input_labels.astype(np.float32) # adapt to pytorch input format\n",
    "input_labels.shape\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(input_features, input_labels, test_size=0.25, random_state=42)\n",
    "train_features, val_features, train_labels, val_labels =  train_test_split(train_features, train_labels, test_size=0.25, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "e1a70975",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(list(zip(train_features, train_labels)), batch_size=4, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(list(zip(val_features, val_labels)), batch_size=4, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(list(zip(test_features, test_labels)), batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "3e5f35dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(4, 32, 12)\n",
    "        self.pool = nn.MaxPool1d(4)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(288, 16)\n",
    "        self.fc2 = nn.Linear(16, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x), dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "b5a9c329",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "b4767858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "num_epochs = 50\n",
    "num_classes = 1\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "ea09230f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "c117bc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Step [100/282], Loss: 0.0024\n",
      "Epoch [1/50], Step [200/282], Loss: 0.0038\n",
      "Epoch [2/50], Step [100/282], Loss: 0.0031\n",
      "Epoch [2/50], Step [200/282], Loss: 0.0084\n",
      "Epoch [3/50], Step [100/282], Loss: 0.0005\n",
      "Epoch [3/50], Step [200/282], Loss: 0.0017\n",
      "Epoch [4/50], Step [100/282], Loss: 0.0006\n",
      "Epoch [4/50], Step [200/282], Loss: 0.0002\n",
      "Epoch [5/50], Step [100/282], Loss: 0.0001\n",
      "Epoch [5/50], Step [200/282], Loss: 0.0000\n",
      "Epoch [6/50], Step [100/282], Loss: 0.0002\n",
      "Epoch [6/50], Step [200/282], Loss: 0.0001\n",
      "Epoch [7/50], Step [100/282], Loss: 0.0003\n",
      "Epoch [7/50], Step [200/282], Loss: 0.0002\n",
      "Epoch [8/50], Step [100/282], Loss: 0.0009\n",
      "Epoch [8/50], Step [200/282], Loss: 0.0006\n",
      "Epoch [9/50], Step [100/282], Loss: 0.0001\n",
      "Epoch [9/50], Step [200/282], Loss: 0.0003\n",
      "Epoch [10/50], Step [100/282], Loss: 0.0001\n",
      "Epoch [10/50], Step [200/282], Loss: 0.0002\n",
      "Epoch [11/50], Step [100/282], Loss: 0.0004\n",
      "Epoch [11/50], Step [200/282], Loss: 0.0004\n",
      "Epoch [12/50], Step [100/282], Loss: 0.0000\n",
      "Epoch [12/50], Step [200/282], Loss: 0.0000\n",
      "Epoch [13/50], Step [100/282], Loss: 0.0000\n",
      "Epoch [13/50], Step [200/282], Loss: 0.0003\n",
      "Epoch [14/50], Step [100/282], Loss: 0.0002\n",
      "Epoch [14/50], Step [200/282], Loss: 0.0000\n",
      "Epoch [15/50], Step [100/282], Loss: 0.0000\n",
      "Epoch [15/50], Step [200/282], Loss: 0.0001\n",
      "Epoch [16/50], Step [100/282], Loss: 0.0001\n",
      "Epoch [16/50], Step [200/282], Loss: 0.0000\n",
      "Epoch [17/50], Step [100/282], Loss: 0.0000\n",
      "Epoch [17/50], Step [200/282], Loss: 0.0000\n",
      "Epoch [18/50], Step [100/282], Loss: 0.0002\n",
      "Epoch [18/50], Step [200/282], Loss: 0.0000\n",
      "Epoch [19/50], Step [100/282], Loss: 0.0000\n",
      "Epoch [19/50], Step [200/282], Loss: 0.0000\n",
      "Epoch [20/50], Step [100/282], Loss: 0.0004\n",
      "Epoch [20/50], Step [200/282], Loss: 0.0002\n",
      "Epoch [21/50], Step [100/282], Loss: 0.0001\n",
      "Epoch [21/50], Step [200/282], Loss: 0.0000\n",
      "Epoch [22/50], Step [100/282], Loss: 0.0000\n",
      "Epoch [22/50], Step [200/282], Loss: 0.0000\n",
      "Epoch [23/50], Step [100/282], Loss: 0.0000\n",
      "Epoch [23/50], Step [200/282], Loss: 0.0000\n",
      "Epoch [24/50], Step [100/282], Loss: 0.0000\n",
      "Epoch [24/50], Step [200/282], Loss: 0.0000\n",
      "Epoch [25/50], Step [100/282], Loss: 0.0000\n",
      "Epoch [25/50], Step [200/282], Loss: 0.0000\n",
      "Epoch [26/50], Step [100/282], Loss: 0.0000\n",
      "Epoch [26/50], Step [200/282], Loss: 0.0000\n",
      "Epoch [27/50], Step [100/282], Loss: 0.0000\n",
      "Epoch [27/50], Step [200/282], Loss: 0.0000\n",
      "Epoch [28/50], Step [100/282], Loss: 0.0000\n",
      "Epoch [28/50], Step [200/282], Loss: 0.0000\n",
      "Epoch [29/50], Step [100/282], Loss: 0.0000\n",
      "Epoch [29/50], Step [200/282], Loss: 0.0000\n",
      "Epoch [30/50], Step [100/282], Loss: 0.0000\n",
      "Epoch [30/50], Step [200/282], Loss: 0.0000\n",
      "Epoch [31/50], Step [100/282], Loss: 0.0000\n",
      "Epoch [31/50], Step [200/282], Loss: 0.0000\n",
      "Epoch [32/50], Step [100/282], Loss: 0.0000\n",
      "Epoch [32/50], Step [200/282], Loss: 0.0000\n",
      "Epoch [33/50], Step [100/282], Loss: 0.0000\n",
      "Epoch [33/50], Step [200/282], Loss: 0.0000\n",
      "Epoch [34/50], Step [100/282], Loss: 0.0000\n",
      "Epoch [34/50], Step [200/282], Loss: 0.0000\n",
      "Epoch [35/50], Step [100/282], Loss: 0.0000\n",
      "Epoch [35/50], Step [200/282], Loss: 0.0000\n",
      "Epoch [36/50], Step [100/282], Loss: 0.0000\n",
      "Epoch [36/50], Step [200/282], Loss: 0.0000\n",
      "Epoch [37/50], Step [100/282], Loss: 0.0000\n",
      "Epoch [37/50], Step [200/282], Loss: 0.0000\n",
      "Epoch [38/50], Step [100/282], Loss: 0.0000\n",
      "Epoch [38/50], Step [200/282], Loss: 0.0000\n",
      "Epoch [39/50], Step [100/282], Loss: 0.0000\n",
      "Epoch [39/50], Step [200/282], Loss: 0.0000\n",
      "Epoch [40/50], Step [100/282], Loss: 0.0000\n",
      "Epoch [40/50], Step [200/282], Loss: 0.0000\n",
      "Epoch [41/50], Step [100/282], Loss: 0.0000\n",
      "Epoch [41/50], Step [200/282], Loss: 0.0000\n",
      "Epoch [42/50], Step [100/282], Loss: 0.0000\n",
      "Epoch [42/50], Step [200/282], Loss: 0.0000\n",
      "Epoch [43/50], Step [100/282], Loss: 0.0000\n",
      "Epoch [43/50], Step [200/282], Loss: 0.0000\n",
      "Epoch [44/50], Step [100/282], Loss: 0.0000\n",
      "Epoch [44/50], Step [200/282], Loss: 0.0000\n",
      "Epoch [45/50], Step [100/282], Loss: 0.0000\n",
      "Epoch [45/50], Step [200/282], Loss: 0.0000\n",
      "Epoch [46/50], Step [100/282], Loss: 0.0000\n",
      "Epoch [46/50], Step [200/282], Loss: 0.0000\n",
      "Epoch [47/50], Step [100/282], Loss: 0.0000\n",
      "Epoch [47/50], Step [200/282], Loss: 0.0000\n",
      "Epoch [48/50], Step [100/282], Loss: 0.0000\n",
      "Epoch [48/50], Step [200/282], Loss: 0.0000\n",
      "Epoch [49/50], Step [100/282], Loss: 0.0000\n",
      "Epoch [49/50], Step [200/282], Loss: 0.0000\n",
      "Epoch [50/50], Step [100/282], Loss: 0.0000\n",
      "Epoch [50/50], Step [200/282], Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "063b9041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv1d(4, 32, kernel_size=(12,), stride=(1,))\n",
       "  (pool): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=288, out_features=16, bias=True)\n",
       "  (fc2): Linear(in_features=16, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "a9864847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "4dbe8cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d46e60",
   "metadata": {},
   "source": [
    "## Old Tensorflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfd82967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17a2bc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 3001, 4)]         0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 2997, 15)          315       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 2996, 3)           93        \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2996, 1)           4         \n",
      "=================================================================\n",
      "Total params: 412\n",
      "Trainable params: 412\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, MaxPooling1D, Conv1D, GlobalMaxPool1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "model_in = Input(shape=(train_features_1[1].shape))\n",
    "model = (Conv1D(filters = 15, kernel_size = 5))(model_in)\n",
    "\n",
    "model1 = (Conv1D(filters = 3, kernel_size = 2))(model)\n",
    "# model1 = (MaxPooling1D(pool_size=(7)))(model1)\n",
    "# model1 =(Dense(14, activation='relu'))(model1)\n",
    "# model1 = (MaxPooling1D(pool_size=(7)))(model1)\n",
    "model1 = (Dense(1, activation='relu'))(model1)\n",
    "\n",
    "model = Model(model_in, model1)\n",
    "model.compile(loss='binary_crossentropy',optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8544953",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae1a82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "a71cf7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyp = [29, 19, 2, 39, 15, 3]\n",
    "hyp = [15, 5, 15, 2, 7, 14]\n",
    "\n",
    "model_to_test = run_model(build_model(hyp))\n",
    "\n",
    "print(average_percentage_error(model_to_test))\n",
    "\n",
    "# plt.title('first: f ='+str(hyp[0])+'k ='+ str(hyp[1]) + '\\n second: f ='+ str(hyp[2]) + 'k =' + str(hyp[3]) +\n",
    "#          '\\n pool size =' + str(hyp[4]) + '\\n Dense layer =' + str(hyp[5]))\n",
    "# plt.scatter(test_labels['mutant_average'], test_labels['CRISPRoff_average'], c='black',s = 0.5)\n",
    "# plt.scatter(predicted[0], predicted[1], c='red',s = 0.1)\n",
    "# plt.figure()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "51e8c300",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/sergiomares/Desktop/DeepOff/DeepOff - Positional Methylations.ipynb Cell 21'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sergiomares/Desktop/DeepOff/DeepOff%20-%20Positional%20Methylations.ipynb#ch0000037?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvis_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m plot_model\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sergiomares/Desktop/DeepOff/DeepOff%20-%20Positional%20Methylations.ipynb#ch0000037?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpydot\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sergiomares/Desktop/DeepOff/DeepOff%20-%20Positional%20Methylations.ipynb#ch0000037?line=3'>4</a>\u001b[0m plot_model(model, to_file\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmodel.png\u001b[39m\u001b[39m'\u001b[39m, show_shapes\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,show_layer_names\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Conv1D, Dense, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=32, kernel_size=12, \n",
    "                 input_shape=(train_features.shape[1], 4)))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', \n",
    "              metrics=['binary_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "58d4a700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def build_model(list):\n",
    "    model_in = Input(shape=(train_features[1].shape))\n",
    "    model = (Conv1D(filters = list[0], kernel_size = list[1], padding = 'same'))(model_in)\n",
    "\n",
    "    model1 = (Conv1D(filters = list[2], kernel_size = list[3], padding = 'same'))(model)\n",
    "    model1 = (MaxPooling1D(pool_size=(list[4])))(model1)\n",
    "    model1 =(Dense(list[5], activation='relu'))(model1)\n",
    "    model1 = GlobalMaxPool1D()(model1)\n",
    "    model1 = (Dense(1, activation='relu'))(model1)\n",
    "\n",
    "    model2 = (Conv1D(filters = list[2], kernel_size = list[3], padding = 'same'))(model)\n",
    "    model2 = (MaxPooling1D(pool_size=(list[4])))(model2)\n",
    "    model2 =(Dense(list[5], activation='relu'))(model2)\n",
    "    model2 = GlobalMaxPool1D()(model2)\n",
    "    model2 = (Dense(1, activation='relu'))(model2)\n",
    "\n",
    "    model = Model(model_in, [model1,model2])\n",
    "    model.compile(loss='binary_crossentropy',optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def run_model(model):\n",
    "    tf.random.set_seed(2)\n",
    "    model.fit(train_features, train_labels, batch_size=50, verbose=0, validation_split=0.35)\n",
    "    predicted = model.predict(test_features)\n",
    "\n",
    "    return(predicted)\n",
    "\n",
    "\n",
    "def average_percentage_error(predicted):\n",
    "\n",
    "    predicted_values[0], predicted_values[1] = pd.DataFrame(predicted[0]), pd.DataFrame(predicted[1])\n",
    "\n",
    "    values = (0, 0)\n",
    "\n",
    "    for i in range(4674):\n",
    "        temp = abs(test_labels[i] - predicted_values[0][i])/test_labels[i] * 100\n",
    "        values = values[0] + temp\n",
    "    \n",
    "    values =  values[0]/4674, values[1]/4674\n",
    "\n",
    "    return values\n",
    "\n",
    "\n",
    "def good_parameter(evaluation, previous_evaluation, parameter_list, index_of_parameter, best_list):\n",
    "\n",
    "    if evaluation[0] < previous_evaluation[0] and evaluation[1] < previous_evaluation[1]:\n",
    "        best_list = parameter_list\n",
    "        if index_of_parameter < 5:\n",
    "            index_of_parameter = index_of_parameter + 1\n",
    "        else:\n",
    "            index_of_parameter = 0 \n",
    "        parameter_list[index_of_parameter] = parameter_list[index_of_parameter] + 1\n",
    "        return(parameter_list, index_of_parameter, evaluation, best_list)\n",
    "    else:\n",
    "        parameter_list[index_of_parameter] = parameter_list[index_of_parameter] - 1\n",
    "        if index_of_parameter < 5:\n",
    "            index_of_parameter = index_of_parameter + 1\n",
    "        else:\n",
    "            index_of_parameter = 0 \n",
    "        parameter_list[index_of_parameter] = parameter_list[index_of_parameter] + 1\n",
    "        return(best_list, index_of_parameter, evaluation, best_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1816,
   "id": "7530b202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1] (64.22614452424135, -237.41724892290424)\n",
      "[1, 2, 1, 1, 1, 1] (60.64750274623421, -245.3171381356713)\n",
      "[1, 2, 2, 1, 1, 1] (405.1922642618783, 432.4582563950403)\n",
      "[1, 2, 1, 2, 1, 1] (77.05839871811408, -224.4955554711991)\n",
      "[1, 2, 1, 2, 2, 1] (19.86221879391772, -224.4041814285325)\n",
      "[1, 2, 1, 2, 1, 2] (318.86443409784005, 14.833894841594951)\n",
      "[2, 2, 1, 2, 1, 1] (27.83288400019977, -218.90666303906593)\n",
      "[2, 3, 1, 2, 1, 1] (91.07822817933027, -236.1995557940928)\n",
      "[2, 2, 2, 2, 1, 1] (1030.1156287669253, 1159.786533572236)\n",
      "[2, 2, 1, 3, 1, 1] (77.51905565813108, -232.2806624464456)\n",
      "[2, 2, 1, 3, 2, 1] (45.40877994749933, -237.95664829707195)\n",
      "[2, 2, 1, 3, 2, 2] (175.4703429280843, -132.88822822165844)\n",
      "[3, 2, 1, 3, 2, 1] (157.89961574552467, -155.16530728234795)\n",
      "[3, 3, 1, 3, 2, 1] (150.86176656745224, -162.7207603041176)\n",
      "[3, 3, 2, 3, 2, 1] (1753.7591985857143, 1884.7218743787057)\n",
      "[3, 3, 1, 4, 2, 1] (131.73103138429298, -193.3596846793653)\n",
      "[3, 3, 1, 4, 3, 1] (129.25506270111092, -197.98980254299354)\n",
      "[3, 3, 1, 4, 3, 2] (245.96177236997778, -4.665107633099732)\n",
      "[4, 3, 1, 4, 3, 1] (118.27329595093568, -215.28101399692346)\n",
      "[4, 4, 1, 4, 3, 1] (146.8875931548081, -179.91679394466004)\n",
      "[4, 3, 2, 4, 3, 1] (423.19922329958047, 353.27780007214926)\n",
      "[4, 3, 1, 5, 3, 1] (124.79549579892485, -207.40993844357658)\n",
      "[4, 3, 1, 5, 4, 1] (99.27045237565915, -240.68137719666026)\n",
      "[4, 3, 1, 5, 4, 2] (186.8734524091362, -117.10316759168111)\n",
      "[5, 3, 1, 5, 4, 1] (98.7988314024618, -242.08848307510624)\n",
      "[5, 4, 1, 5, 4, 1] (65.04696938971414, -250.74864537083488)\n",
      "[5, 4, 2, 5, 4, 1] (723.2614106575359, 485.86408501785263)\n",
      "[5, 4, 1, 6, 4, 1] (40.31344093919785, -224.17014935287241)\n",
      "[5, 4, 1, 6, 5, 1] (26.41796402188796, -226.61298529193687)\n",
      "[5, 4, 1, 6, 5, 2] (181.99815658079388, -116.06466390577957)\n",
      "[6, 4, 1, 6, 5, 1] (145.77641444419504, -181.24220845577636)\n",
      "[6, 5, 1, 6, 5, 1] (105.91485502273702, -229.4069724750926)\n",
      "[6, 5, 2, 6, 5, 1] (274.21877552247736, -9.889494050059367)\n",
      "[6, 5, 1, 7, 5, 1] (135.67565945827678, -191.65721834381222)\n",
      "[6, 5, 1, 7, 6, 1] (134.37258620366777, -197.05501026271492)\n",
      "[6, 5, 1, 7, 6, 2] (249.49792967719665, 1.1640728139753076)\n",
      "[7, 5, 1, 7, 6, 1] (84.45023045821249, -246.5143561581655)\n",
      "[7, 6, 1, 7, 6, 1] (62.00517453450457, -238.76826176026114)\n",
      "[7, 5, 2, 7, 6, 1] (482.8037008924229, 454.29760999450247)\n",
      "[7, 5, 1, 8, 6, 1] (73.91741578203447, -246.05382193094982)\n",
      "[7, 5, 1, 8, 7, 1] (63.07764258777084, -239.43984021288736)\n",
      "[7, 5, 1, 8, 6, 2] (196.9309499817055, -94.4421726019488)\n",
      "[8, 5, 1, 8, 6, 1] (38.631408595839986, -242.87673200200615)\n",
      "[8, 6, 1, 8, 6, 1] (52.839904568668494, -242.77868884269)\n",
      "[8, 5, 2, 8, 6, 1] (415.5765308610413, 346.5433552127908)\n",
      "[8, 5, 1, 9, 6, 1] (30.98501068247994, -239.27250102313866)\n",
      "[8, 5, 1, 9, 7, 1] (62.083352752703185, -248.2571266423384)\n",
      "[8, 5, 1, 9, 6, 2] (221.22183456103753, -65.79843482783137)\n",
      "[9, 5, 1, 9, 6, 1] (42.18128169323941, -238.40328308118984)\n",
      "[9, 6, 1, 9, 6, 1] (44.355863224240466, -236.5101209136238)\n",
      "[9, 5, 2, 9, 6, 1] (353.9429495945708, 169.79191764019117)\n",
      "[9, 5, 1, 10, 6, 1] (50.93060545803645, -236.1101214146421)\n",
      "[9, 5, 1, 10, 7, 1] (49.19128594770734, -242.34759540573893)\n",
      "[9, 5, 1, 10, 7, 2] (164.85300931764942, -149.57793929507588)\n",
      "[10, 5, 1, 10, 7, 1] (66.18456248684437, -247.6590894224472)\n",
      "[10, 6, 1, 10, 7, 1] (86.40045291737594, -252.96430665215325)\n",
      "[10, 5, 2, 10, 7, 1] (383.380206671082, 290.9780686232094)\n",
      "[10, 5, 1, 11, 7, 1] (37.19713935093243, -240.37700270162233)\n",
      "[10, 5, 1, 11, 8, 1] (33.591616917221586, -246.73370206778094)\n",
      "[10, 5, 1, 11, 8, 2] (242.04669838400648, 39.75031714505909)\n",
      "[11, 5, 1, 11, 8, 1] (29.122213902802482, -232.65092695559417)\n",
      "[11, 6, 1, 11, 8, 1] (28.673961695322387, -239.64084885376926)\n",
      "[11, 6, 2, 11, 8, 1] (335.3077338639178, 234.7431630033792)\n",
      "[11, 6, 1, 12, 8, 1] (32.23260927937519, -229.80929970280886)\n",
      "[11, 6, 1, 12, 9, 1] (36.57035677211503, -236.32660760378116)\n",
      "[11, 6, 1, 12, 8, 2] (219.00947870991408, -39.41589398111935)\n",
      "[12, 6, 1, 12, 8, 1] (117.98380953630696, -209.76392085561812)\n",
      "[12, 7, 1, 12, 8, 1] (95.85792973347678, -234.19816931455566)\n",
      "[12, 7, 2, 12, 8, 1] (386.34438135221865, 172.46182264175565)\n",
      "[12, 7, 1, 13, 8, 1] (85.707791452449, -244.0558037361863)\n",
      "[12, 7, 1, 13, 9, 1] (87.32715759560882, -241.02576233681256)\n",
      "[12, 7, 1, 13, 8, 2] (321.73792986179427, 120.23196121829926)\n",
      "[13, 7, 1, 13, 8, 1] (54.85356910962733, -237.91493540918506)\n",
      "[13, 8, 1, 13, 8, 1] (46.25085727011219, -237.3267555210422)\n",
      "[13, 7, 2, 13, 8, 1] (1183.264325628323, 1197.5636065477229)\n",
      "[13, 7, 1, 14, 8, 1] (60.858436079843855, -239.39336997003102)\n",
      "[13, 7, 1, 14, 9, 1] (79.26552541736163, -250.28074029784804)\n",
      "[13, 7, 1, 14, 8, 2] (239.67847265777235, -20.51246429342708)\n",
      "[14, 7, 1, 14, 8, 1] (56.76789612719548, -239.26163985624183)\n",
      "[14, 8, 1, 14, 8, 1] (68.46245629676804, -231.95569590813503)\n",
      "[14, 7, 2, 14, 8, 1] (388.71541858556384, 279.83782322014997)\n",
      "[14, 7, 1, 15, 8, 1] (66.54988411986106, -237.15850319047465)\n",
      "[14, 7, 1, 15, 9, 1] (130.17651826892524, -202.5531654605628)\n",
      "[14, 7, 1, 15, 8, 2] (329.3604124118455, 124.13228997873838)\n",
      "[15, 7, 1, 15, 8, 1] (38.30130856026133, -231.92488728041238)\n",
      "[15, 8, 1, 15, 8, 1] (49.107501129704545, -230.73357787764306)\n",
      "[15, 7, 2, 15, 8, 1] (652.7706743849795, 708.5755311663413)\n",
      "[15, 7, 1, 16, 8, 1] (40.07821873319287, -229.51742499715414)\n",
      "[15, 7, 1, 16, 9, 1] (36.16702718501135, -231.72399783210034)\n",
      "[15, 7, 1, 16, 9, 2] (185.15021088342533, -101.68126425965346)\n",
      "[16, 7, 1, 16, 9, 1] (132.7342216421363, -196.50035811602947)\n",
      "[16, 8, 1, 16, 9, 1] (105.7718303236952, -224.93451286713494)\n",
      "[16, 8, 2, 16, 9, 1] (645.036527619878, 638.5249269743784)\n",
      "[16, 8, 1, 17, 9, 1] (119.93255502490652, -207.16543624742604)\n",
      "[16, 8, 1, 17, 10, 1] (113.53841093731282, -216.08355464349864)\n",
      "[16, 8, 1, 17, 10, 2] (224.81454498167167, -26.317284282717072)\n",
      "[17, 8, 1, 17, 10, 1] (92.95304591341852, -241.0176641549226)\n",
      "[17, 9, 1, 17, 10, 1] (78.78583240914126, -248.8558658063486)\n",
      "[17, 9, 2, 17, 10, 1] (520.9317424557956, 451.51592768142604)\n",
      "[17, 9, 1, 18, 10, 1] (105.15117505725165, -227.79741103796948)\n",
      "[17, 9, 1, 18, 11, 1] (102.36137475492038, -234.02041446725656)\n",
      "[17, 9, 1, 18, 11, 2] (186.06399373665784, -117.60941551748515)\n",
      "[18, 9, 1, 18, 11, 1] (120.58419897003354, -208.36166599955553)\n",
      "[18, 10, 1, 18, 11, 1] (112.90877803244723, -220.92658669286106)\n",
      "[18, 10, 2, 18, 11, 1] (226.26936704259134, 146.9701864647728)\n",
      "[18, 10, 1, 19, 11, 1] (120.65439435214994, -212.09912523791957)\n",
      "[18, 10, 1, 19, 12, 1] (118.46266336738333, -215.60388281294897)\n",
      "[18, 10, 1, 19, 12, 2] (159.41985394944635, -163.61690816051535)\n",
      "[19, 10, 1, 19, 12, 1] (18.800160825098995, -235.01226757703006)\n",
      "[19, 11, 1, 19, 12, 1] (61.02020739496981, -247.95346453215072)\n",
      "[19, 10, 2, 19, 12, 1] (407.1732176763893, 385.3164338331293)\n",
      "[19, 10, 1, 20, 12, 1] (42.33925500971783, -238.60094978288015)\n",
      "[19, 10, 1, 20, 13, 1] (42.06964714231206, -238.24529906322508)\n",
      "[19, 10, 1, 20, 12, 2] (196.55920475375385, -99.0827544205123)\n",
      "[20, 10, 1, 20, 12, 1] (120.87263754924163, -214.58925052527786)\n",
      "[20, 11, 1, 20, 12, 1] (110.02932777830482, -221.80114464582942)\n",
      "[20, 11, 2, 20, 12, 1] (540.4425887132453, 547.2992375566407)\n",
      "[20, 11, 1, 21, 12, 1] (120.19402747685793, -212.13090946323732)\n",
      "[20, 11, 1, 21, 13, 1] (105.5295811060794, -227.17061844931084)\n",
      "[20, 11, 1, 21, 13, 2] (160.36495631634153, -148.67388082605206)\n",
      "[21, 11, 1, 21, 13, 1] (56.58205518472649, -244.1155987989598)\n",
      "[21, 12, 1, 21, 13, 1] (40.074643841802754, -239.65672745141197)\n",
      "[21, 11, 2, 21, 13, 1] (300.69617382281723, 164.06510312802143)\n",
      "[21, 11, 1, 22, 13, 1] (59.861964401045796, -244.76231890226512)\n",
      "[21, 11, 1, 22, 14, 1] (54.90019057082985, -245.54188873760089)\n",
      "[21, 11, 1, 22, 14, 2] (194.0204877402389, -95.60016118466109)\n",
      "[22, 11, 1, 22, 14, 1] (74.30483757754223, -244.20812578473385)\n",
      "[22, 12, 1, 22, 14, 1] (50.91382077235794, -239.44532607757412)\n",
      "[22, 11, 2, 22, 14, 1] (558.877394826053, 521.0859133487621)\n",
      "[22, 11, 1, 23, 14, 1] (84.14532530203917, -245.5875057513656)\n",
      "[22, 11, 1, 23, 15, 1] (88.16035109977946, -244.8555868989207)\n",
      "[22, 11, 1, 23, 14, 2] (149.93370488910705, -169.24821580749108)\n",
      "[23, 11, 1, 23, 14, 1] (94.82252055476286, -242.7818146339042)\n",
      "[23, 12, 1, 23, 14, 1] (103.0701984369629, -231.54974665519308)\n",
      "[23, 11, 2, 23, 14, 1] (401.38432439013644, 250.4553468972334)\n",
      "[23, 11, 1, 24, 14, 1] (74.97415674445318, -247.1285631001637)\n",
      "[23, 11, 1, 24, 15, 1] (72.81498661598003, -247.16398558156214)\n",
      "[23, 11, 1, 24, 15, 2] (136.3792648214789, -184.96826680206624)\n",
      "[24, 11, 1, 24, 15, 1] (104.2315974617033, -231.53797731047703)\n",
      "[24, 12, 1, 24, 15, 1] (108.847340435852, -227.8708486838687)\n",
      "[24, 11, 2, 24, 15, 1] (462.7442136215034, 385.1362418277951)\n",
      "[24, 11, 1, 25, 15, 1] (62.53888983817092, -240.30723980431623)\n",
      "[24, 11, 1, 25, 16, 1] (109.48133431162985, -227.6628862612153)\n",
      "[24, 11, 1, 25, 15, 2] (223.17764982391571, -37.312269098001906)\n",
      "[25, 11, 1, 25, 15, 1] (133.91023254649693, -195.67310517606393)\n",
      "[25, 12, 1, 25, 15, 1] (132.68065986772757, -200.174565649238)\n",
      "[25, 12, 2, 25, 15, 1] (723.2564976705136, 806.3538002141363)\n",
      "[25, 12, 1, 26, 15, 1] (133.4153807263017, -200.19895443980366)\n",
      "[25, 12, 1, 26, 16, 1] (139.0688251111114, -192.53393595210753)\n",
      "[25, 12, 1, 26, 15, 2] (186.56532858075946, -82.89403074143974)\n",
      "[26, 12, 1, 26, 15, 1] (111.05235952100418, -222.96964123168127)\n",
      "[26, 13, 1, 26, 15, 1] (120.49307947189709, -213.7102729540066)\n",
      "[26, 12, 2, 26, 15, 1] (240.08221035569179, 107.01415495237794)\n",
      "[26, 12, 1, 27, 15, 1] (119.93867364412196, -213.92708281643755)\n",
      "[26, 12, 1, 27, 16, 1] (123.02027757395838, -210.25341546845016)\n",
      "[26, 12, 1, 27, 15, 2] (148.2911262799753, -169.93411182850804)\n",
      "[27, 12, 1, 27, 15, 1] (117.83024121108897, -209.09277294430998)\n",
      "[27, 13, 1, 27, 15, 1] (126.72673513521396, -201.58255352985668)\n",
      "[27, 12, 2, 27, 15, 1] (1062.3048236355132, 1183.737678015083)\n",
      "[27, 12, 1, 28, 15, 1] (124.09492486100582, -202.91171918917237)\n",
      "[27, 12, 1, 28, 16, 1] (123.93567815316507, -206.3818749162113)\n",
      "[27, 12, 1, 28, 16, 2] (255.11523196434425, 63.90804901053827)\n",
      "[28, 12, 1, 28, 16, 1] (129.29089457946603, -202.6276197125482)\n",
      "[28, 13, 1, 28, 16, 1] (127.60172838229455, -199.3212258809301)\n",
      "[28, 12, 2, 28, 16, 1] (756.1738085483612, 772.104969971987)\n",
      "[28, 12, 1, 29, 16, 1] (119.55677482880121, -215.29469962981656)\n",
      "[28, 12, 1, 29, 17, 1] (117.39542615804793, -214.4202998615643)\n",
      "[28, 12, 1, 29, 16, 2] (320.1187549521262, 143.95196649536754)\n",
      "[29, 12, 1, 29, 16, 1] (126.44761856064599, -208.60059897904785)\n",
      "[29, 13, 1, 29, 16, 1] (126.7061867685523, -208.47643922973612)\n",
      "[29, 12, 2, 29, 16, 1] (1086.1972291474592, 1209.0236621990803)\n",
      "[29, 12, 1, 30, 16, 1] (132.85018055629484, -201.7018658122097)\n",
      "[29, 12, 1, 30, 17, 1] (129.64110484191573, -201.94965523187776)\n",
      "[29, 12, 1, 30, 17, 2] (249.0769131369346, 25.421930694891497)\n",
      "[30, 12, 1, 30, 17, 1] (130.91744745717986, -199.38399048705597)\n",
      "[30, 13, 1, 30, 17, 1] (136.49446714987565, -189.3362803890948)\n",
      "[30, 12, 2, 30, 17, 1] (279.130613712575, 44.07518550359504)\n",
      "[30, 12, 1, 31, 17, 1] (136.23634597132298, -194.1723062238066)\n",
      "[30, 12, 1, 31, 18, 1] (131.98485928957658, -198.9182850718776)\n",
      "[30, 12, 1, 31, 18, 2] (330.9980741519919, 155.20661254520667)\n",
      "[31, 12, 1, 31, 18, 1] (149.10956040900524, -177.75885811433687)\n",
      "[31, 13, 1, 31, 18, 1] (142.12450897548166, -186.34668566029566)\n",
      "[31, 13, 2, 31, 18, 1] (345.9721331565339, 307.2609271255245)\n",
      "[31, 13, 1, 32, 18, 1] (152.6324784917993, -168.24152590857832)\n",
      "[31, 13, 1, 32, 19, 1] (150.98665338610027, -174.53295457204013)\n",
      "[31, 13, 1, 32, 19, 2] (338.9501007294154, 182.28739289868741)\n",
      "[32, 13, 1, 32, 19, 1] (133.14424411022608, -191.79969154689314)\n",
      "[32, 14, 1, 32, 19, 1] (130.58846254559126, -197.0683653580804)\n",
      "[32, 14, 2, 32, 19, 1] (825.6648078990213, 899.9382884817425)\n",
      "[32, 14, 1, 33, 19, 1] (120.00030910074504, -207.8187066309729)\n",
      "[32, 14, 1, 33, 20, 1] (121.2509351647586, -207.6061054362227)\n",
      "[32, 14, 1, 33, 19, 2] (205.99650112578078, -111.06194054730727)\n",
      "[33, 14, 1, 33, 19, 1] (131.34086748244468, -205.8723632813318)\n",
      "[33, 15, 1, 33, 19, 1] (125.51646942281883, -207.7871739567573)\n",
      "[33, 15, 2, 33, 19, 1] (296.5063667297342, 189.02566128265858)\n",
      "[33, 15, 1, 34, 19, 1] (119.4609744190879, -215.98274267553916)\n",
      "[33, 15, 1, 34, 20, 1] (113.9969351977355, -219.59282544899256)\n",
      "[33, 15, 1, 34, 20, 2] (217.64029124841852, -71.14041223332529)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pj/0dd0rqzj219164nwyh4f2l580000gn/T/ipykernel_1639/4019852725.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m#return evaluation of the model (2 values)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0meva\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maverage_percentage_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meva\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/pj/0dd0rqzj219164nwyh4f2l580000gn/T/ipykernel_1639/3414217300.py\u001b[0m in \u001b[0;36maverage_percentage_error\u001b[0;34m(predicted)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4674\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mutant_average'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpredicted_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mutant_average'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mtemp2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CRISPRoff_average'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpredicted_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mutant_average'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtemp2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_values = []\n",
    "model_value = []\n",
    "\n",
    "#THIS ONE IS DONE WITH THE SIGMOID BEFORE THE LEAKYRELU\n",
    "\n",
    "#hp = [2, 1, 2, 3, 2, 3]\n",
    "hp = [1,1,1,1,1,1]\n",
    "pe = (1000, 1000)\n",
    "bl = hp\n",
    "\n",
    "for so_many_models in range(1000):\n",
    "    \n",
    "    for each_parameters in range(6): # for each parameter\n",
    "    \n",
    "        #return predicted model\n",
    "        running = run_model(build_model(hp)) \n",
    "\n",
    "        #return evaluation of the model (2 values)\n",
    "        eva = average_percentage_error(running) \n",
    "\n",
    "        print(hp, eva)\n",
    "\n",
    "        #compare model with previous, return parameter list and index\n",
    "        hp, each_parameters, pe, bl = good_parameter(eva, pe, hp, each_parameters, bl)\n",
    "\n",
    "        # predicted = run_model(build_model(hp))\n",
    "\n",
    "        # plt.title('first: f ='+str(hp[0])+'k ='+ str(hp[1]) + '\\n second: f ='+ str(hp[2]) + 'k =' + str(hp[3]) +\n",
    "        #  '\\n pool size =' + str(hp[4]) + '\\n Dense layer =' + str(hp[5]))\n",
    "        # plt.scatter(test_labels['mutant_average'], test_labels['CRISPRoff_average'], c='black',s = 0.1)\n",
    "        # plt.scatter(predicted[0], predicted[1], c='red',s = 0.1)\n",
    "        # plt.figure()\n",
    "        # plt.show()\n",
    "\n",
    "        if abs(eva[0]) < 50 and abs(eva[1]) < 50:\n",
    "            print('We fucking got a motherfucking good model bitch', eva, hp)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
